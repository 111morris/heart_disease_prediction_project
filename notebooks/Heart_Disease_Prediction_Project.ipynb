{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages \n",
    "!pip install kaggle\n",
    "!pip install pandas numpy scikit-learn matplotlib seaborn\n",
    "\n",
    "# Import all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d35ade",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Direct download from UCI repository\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset directly from the corrected URL\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
    "# The dataset does not have a header row, and the columns are not named.\n",
    "# We need to provide column names manually based on the dataset description.\n",
    "column_names = [\n",
    "    'age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n",
    "    'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'\n",
    "]\n",
    "df = pd.read_csv(url, names=column_names, na_values=\"?\") # Handle missing values represented by '?'\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1778ef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('heart.csv') or you can use the variable from alternative method\n",
    "\n",
    "# 1. View first few rows\n",
    "print(\"=== First 5 rows of the dataset ===\")\n",
    "print(df.head())\n",
    "print(\"\\n\")\n",
    "\n",
    "# 2. Check dataset info (data types, non-null counts)\n",
    "print(\"=== Dataset Information ===\")\n",
    "print(df.info())\n",
    "print(\"\\n\")\n",
    "\n",
    "# 3. Summary statistics\n",
    "print(\"=== Summary Statistics ===\")\n",
    "print(df.describe())\n",
    "print(\"\\n\")\n",
    "\n",
    "# 4. Check for missing values\n",
    "print(\"=== Missing Values Check ===\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\n\")\n",
    "\n",
    "# 5. Check target variable distribution\n",
    "print(\"=== Target Variable Distribution ===\")\n",
    "print(df['target'].value_counts())\n",
    "print(f\"Percentage with heart disease: {(df['target'].sum()/len(df)*100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def6a0e1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check for missing values again after loading with '?' as NA\n",
    "print(\"=== Missing values after loading with na_values='?' ===\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Handle missing values: Fill missing values in 'ca' and 'thal' with the mode\n",
    "for col in ['ca', 'thal']:\n",
    "    if df[col].isnull().any():\n",
    "        mode_value = df[col].mode()[0]\n",
    "        df[col].fillna(mode_value, inplace=True)\n",
    "        print(f\"Filled missing values in '{col}' with the mode: {mode_value}\")\n",
    "\n",
    "print(\"\\n=== Missing values after handling ===\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edec764d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Converts columns to appropriate data types\n",
    "# 'ca' and 'thal' are currently float due to missing values, convert them to int\n",
    "for col in ['ca', 'thal', 'target']:\n",
    "    df[col] = df[col].astype(int)\n",
    "\n",
    "print(\"=== Data types after conversion ===\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fad11e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Explore the distribution of categorical features\n",
    "categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal', 'target']\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(categorical_features):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    df[col].value_counts().plot(kind='bar', color=sns.color_palette('viridis'))\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd1665",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(numerical_features):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    sns.histplot(df[col], kde=True, color=sns.color_palette('viridis')[i])\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72509489",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create a feature dictionary for reference\n",
    "feature_dict = {\n",
    "    'age': 'Age in years',\n",
    "    'sex': 'Sex (1 = male, 0 = female)',\n",
    "    'cp': 'Chest pain type (0-3)',\n",
    "    'trestbps': 'Resting blood pressure (mm Hg)',\n",
    "    'chol': 'Serum cholesterol (mg/dl)',\n",
    "    'fbs': 'Fasting blood sugar > 120 mg/dl (1 = true, 0 = false)',\n",
    "    'restecg': 'Resting ECG results (0-2)',\n",
    "    'thalach': 'Maximum heart rate achieved',\n",
    "    'exang': 'Exercise induced angina (1 = yes, 0 = no)',\n",
    "    'oldpeak': 'ST depression induced by exercise',\n",
    "    'slope': 'Slope of peak exercise ST segment (0-2)',\n",
    "    'ca': 'Number of major vessels colored by fluoroscopy (0-3)',\n",
    "    'thal': 'Thalassemia (0 = normal, 1 = fixed defect, 2 = reversable defect)',\n",
    "    'target': 'Heart disease presence (1 = yes, 0 = no)'\n",
    "}\n",
    "\n",
    "print(\"=== Feature Descriptions ===\")\n",
    "for feature, description in feature_dict.items():\n",
    "    print(f\"{feature}: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6166c2dc",
   "metadata": {},
   "source": [
    "A solution: install kaggle package first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a5f764",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade kaggle\n",
    "!kaggle datasets download -v -d ronitf/heart-disease-uci --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326aac7a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "for fn in uploaded.keys():\n",
    "    print(f'User uploaded file \"{fn}\" with length {len(uploaded[fn])} bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38eed90",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create a simple visualization to understand the data\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Subplot 1: Target distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "df['target'].value_counts().plot(kind='bar', color=['lightcoral', 'lightblue'])\n",
    "plt.title('Heart Disease Distribution')\n",
    "plt.xlabel('Target (0=No Disease, 1=Disease)')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Subplot 2: Age distribution by target\n",
    "plt.subplot(1, 2, 2)\n",
    "df[df['target']==0]['age'].hist(alpha=0.7, label='No Disease', color='lightblue')\n",
    "df[df['target']==1]['age'].hist(alpha=0.7, label='Disease', color='lightcoral')\n",
    "plt.title('Age Distribution by Heart Disease Status')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e22d178",
   "metadata": {},
   "source": [
    "Task 2 Starts: Data preprocessing => cleaning and preparing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb7d5b9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# data quality assessment\n",
    "print(\"=== DATA QUALITY ASSESSMENT ===\\n\")\n",
    "\n",
    "# 1. Check dataset shape\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of patients: {df.shape[0]}\")\n",
    "print(f\"Number of features: {df.shape[1] - 1} (excluding target)\")\n",
    "\n",
    "# 2. Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Duplicate rows: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    print(\"Removing duplicates...\")\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"New shape after removing duplicates: {df.shape}\")\n",
    "\n",
    "# 3. Check for missing values (detailed)\n",
    "print(\"\\n=== Missing Values Analysis ===\")\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "missing_table = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Percentage': missing_percent\n",
    "})\n",
    "print(missing_table[missing_table['Missing Count'] > 0])\n",
    "\n",
    "# 4. Check data types\n",
    "print(\"\\n=== Data Types Check ===\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a52e29",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=== FEATURE ANALYSIS ===\\n\")\n",
    "\n",
    "# Create a detailed feature analysis\n",
    "for column in df.columns:\n",
    "    if column != 'target':\n",
    "        print(f\"\\n--- {column.upper()} ---\")\n",
    "        print(f\"Data type: {df[column].dtype}\")\n",
    "        print(f\"Unique values: {df[column].nunique()}\")\n",
    "        print(f\"Min: {df[column].min()}, Max: {df[column].max()}\")\n",
    "        print(f\"Value counts:\\n{df[column].value_counts().head(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cae85f",
   "metadata": {},
   "source": [
    "# Even though UCI dataset typically has no missing values, \n",
    "# here's how to handle them if they exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b8e56d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def handle_missing_values(df):\n",
    "    \"\"\"Comprehensive missing value handling\"\"\"\n",
    "    print(\"=== HANDLING MISSING VALUES ===\")\n",
    "    \n",
    "    # Check for missing values again\n",
    "    missing = df.isnull().sum()\n",
    "    \n",
    "    if missing.sum() == 0:\n",
    "        print(\"No missing values found!\")\n",
    "        return df\n",
    "    \n",
    "    # Strategy depends on the feature type\n",
    "    for column in df.columns:\n",
    "        if df[column].isnull().sum() > 0:\n",
    "            print(f\"\\nHandling missing values in {column}:\")\n",
    "            \n",
    "            # For numerical features\n",
    "            if df[column].dtype in ['int64', 'float64']:\n",
    "                # Use median (more robust than mean)\n",
    "                median_value = df[column].median()\n",
    "                df[column].fillna(median_value, inplace=True)\n",
    "                print(f\"  - Filled {df[column].isnull().sum()} missing values with median: {median_value}\")\n",
    "            \n",
    "            # For categorical features\n",
    "            else:\n",
    "                # Use mode (most frequent value)\n",
    "                mode_value = df[column].mode()[0]\n",
    "                df[column].fillna(mode_value, inplace=True)\n",
    "                print(f\"  - Filled {df[column].isnull().sum()} missing values with mode: {mode_value}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function\n",
    "df = handle_missing_values(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ba2319",
   "metadata": {},
   "source": [
    "# Outlier detection and handling \n",
    "# Detect outliers using statistical methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82319dab",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def detect_outliers(df, features):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    print(\"=== OUTLIER DETECTION ===\")\n",
    "    \n",
    "    outlier_indices = []\n",
    "    \n",
    "    for feature in features:\n",
    "        if feature != 'target' and df[feature].dtype in ['int64', 'float64']:\n",
    "            # Calculate IQR\n",
    "            Q1 = df[feature].quantile(0.25)\n",
    "            Q3 = df[feature].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            # Define outlier bounds\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            # Find outliers\n",
    "            outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n",
    "            \n",
    "            if len(outliers) > 0:\n",
    "                print(f\"\\n{feature}:\")\n",
    "                print(f\"  - Lower bound: {lower_bound:.2f}\")\n",
    "                print(f\"  - Upper bound: {upper_bound:.2f}\")\n",
    "                print(f\"  - Outliers found: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)\")\n",
    "                \n",
    "                # Visualize outliers\n",
    "                plt.figure(figsize=(10, 4))\n",
    "                \n",
    "                plt.subplot(1, 2, 1)\n",
    "                df[feature].hist(bins=30, alpha=0.7)\n",
    "                plt.axvline(lower_bound, color='red', linestyle='--', label=f'Lower bound: {lower_bound:.1f}')\n",
    "                plt.axvline(upper_bound, color='red', linestyle='--', label=f'Upper bound: {upper_bound:.1f}')\n",
    "                plt.title(f'{feature} Distribution with Outlier Bounds')\n",
    "                plt.legend()\n",
    "                \n",
    "                plt.subplot(1, 2, 2)\n",
    "                df.boxplot(column=feature)\n",
    "                plt.title(f'{feature} Boxplot')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "    \n",
    "    return outlier_indices\n",
    "\n",
    "# Apply outlier detection\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "detect_outliers(df, numerical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61fc696",
   "metadata": {},
   "source": [
    "# Feature engineering and selection\n",
    "# Create new features that might be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ad4a9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"=== FEATURE ENGINEERING ===\")\n",
    "\n",
    "# 1. Age groups\n",
    "df['age_group'] = pd.cut(df['age'], \n",
    "                        bins=[0, 40, 50, 60, 70, 100], \n",
    "                        labels=['<40', '40-50', '50-60', '60-70', '70+'])\n",
    "\n",
    "# 2. Cholesterol categories (using medical standards)\n",
    "df['chol_category'] = pd.cut(df['chol'], \n",
    "                            bins=[0, 200, 240, 1000], \n",
    "                            labels=['Desirable', 'Borderline', 'High'])\n",
    "\n",
    "# 3. Blood pressure categories\n",
    "df['bp_category'] = pd.cut(df['trestbps'], \n",
    "                          bins=[0, 120, 130, 140, 180, 300], \n",
    "                          labels=['Normal', 'Elevated', 'Stage1', 'Stage2', 'Crisis'])\n",
    "\n",
    "# 4. Heart rate efficiency (thalach vs age)\n",
    "df['heart_rate_efficiency'] = df['thalach'] / df['age']\n",
    "\n",
    "print(\"New features created:\")\n",
    "print(\"- age_group: Categorized age ranges\")\n",
    "print(\"- chol_category: Cholesterol levels based on medical standards\")\n",
    "print(\"- bp_category: Blood pressure categories\")\n",
    "print(\"- heart_rate_efficiency: Ratio of max heart rate to age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59b67ab",
   "metadata": {},
   "source": [
    "# Data normalization and scalling\n",
    "# Separate features and target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc82e14",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "print(\"=== FEATURE SCALING ===\")\n",
    "\n",
    "# Identify numerical and categorical features\n",
    "numerical_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numerical features: {len(numerical_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")\n",
    "\n",
    "# Handle categorical features (if any)\n",
    "if categorical_features:\n",
    "    X = pd.get_dummies(X, columns=categorical_features, drop_first=True)\n",
    "    print(f\"After one-hot encoding: {X.shape[1]} features\")\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X.copy()\n",
    "X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "\n",
    "print(\"\\nScaling completed!\")\n",
    "print(f\"Original feature range example (age): {X['age'].min():.1f} to {X['age'].max():.1f}\")\n",
    "print(f\"Scaled feature range example (age): {X_scaled['age'].min():.2f} to {X_scaled['age'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e253d4ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"=== TRAIN-TEST SPLIT ===\")\n",
    "\n",
    "# Split the data (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Training set distribution: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Test set distribution: {y_test.value_counts().to_dict()}\")\n",
    "\n",
    "# Verify the split maintains the class distribution\n",
    "train_ratio = y_train.sum() / len(y_train)\n",
    "test_ratio = y_test.sum() / len(y_test)\n",
    "print(f\"\\nClass balance check:\")\n",
    "print(f\"Training set positive ratio: {train_ratio:.3f}\")\n",
    "print(f\"Test set positive ratio: {test_ratio:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ac65d0",
   "metadata": {},
   "source": [
    "# Final Data verification \n",
    "# Create a comprehensive verification report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c327ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=== FINAL VERIFICATION ===\\n\")\n",
    "\n",
    "def verify_preprocessing(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Verify that preprocessing was successful\"\"\"\n",
    "    \n",
    "    # 1. Check shapes\n",
    "    print(\"1. Shape Verification:\")\n",
    "    print(f\"   X_train: {X_train.shape}\")\n",
    "    print(f\"   X_test: {X_test.shape}\")\n",
    "    print(f\"   y_train: {y_train.shape}\")\n",
    "    print(f\"   y_test: {y_test.shape}\")\n",
    "    \n",
    "    # 2. Check for missing values\n",
    "    print(\"\\n2. Missing Values Check:\")\n",
    "    print(f\"   X_train missing: {X_train.isnull().sum().sum()}\")\n",
    "    print(f\"   X_test missing: {X_test.isnull().sum().sum()}\")\n",
    "    \n",
    "    # 3. Check scaling\n",
    "    print(\"\\n3. Scaling Verification (first 3 numerical features):\")\n",
    "    numerical_cols = X_train.select_dtypes(include=[np.number]).columns[:3]\n",
    "    for col in numerical_cols:\n",
    "        print(f\"   {col}: mean={X_train[col].mean():.3f}, std={X_train[col].std():.3f}\")\n",
    "    \n",
    "    # 4. Check target distribution\n",
    "    print(\"\\n4. Target Distribution:\")\n",
    "    print(f\"   Training: {y_train.value_counts().to_dict()}\")\n",
    "    print(f\"   Testing: {y_test.value_counts().to_dict()}\")\n",
    "    \n",
    "    print(\"\\nâœ… Preprocessing completed successfully!\")\n",
    "\n",
    "# Run verification\n",
    "verify_preprocessing(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b33fb1d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the scaler for later use\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Save the preprocessed data\n",
    "preprocessed_data = {\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'feature_names': X_train.columns.tolist()\n",
    "}\n",
    "\n",
    "with open('preprocessed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessed_data, f)\n",
    "\n",
    "print(\"Preprocessed data saved!\")\n",
    "print(\"Files created: scaler.pkl, preprocessed_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4123f5",
   "metadata": {},
   "source": [
    "# Summary Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64ef700",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the scaler for later use\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Save the preprocessed data\n",
    "preprocessed_data = {\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'feature_names': X_train.columns.tolist()\n",
    "}\n",
    "\n",
    "with open('preprocessed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessed_data, f)\n",
    "\n",
    "print(\"Preprocessed data saved!\")\n",
    "print(\"Files created: scaler.pkl, preprocessed_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc70f2b",
   "metadata": {},
   "source": [
    "# Check if one-hot encoding created too many features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a8746c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Original features: {X.shape[1]}\")\n",
    "print(f\"After preprocessing: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227b7a06",
   "metadata": {},
   "source": [
    "# Task 3: Model Building - Training Multiple Classification Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b7f48",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import additional libraries needed for modeling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "import time\n",
    "\n",
    "print(\"=== MODEL BUILDING SETUP ===\")\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "print(f\"Features: {list(X_train.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e41d7f",
   "metadata": {},
   "source": [
    "# create a model training framework \n",
    "# Create a function to train and evaluate models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcf1965",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Train a model and return performance metrics\n",
    "    \n",
    "    Parameters:\n",
    "    model: sklearn model instance\n",
    "    X_train: training features\n",
    "    y_train: training target\n",
    "    X_test: test features\n",
    "    y_test: test target\n",
    "    model_name: name of the model for display\n",
    "    \n",
    "    Returns:\n",
    "    Dictionary with model results\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Training {model_name} ===\")\n",
    "    \n",
    "    # Record training time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'model': model,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'training_time': training_time\n",
    "    }\n",
    "    \n",
    "    print(f\"Training completed in {training_time:.3f} seconds\")\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"Cross-validation: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a958ea",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Model 1 - Logistic Regression\n",
    "print(\"=== MODEL 1: LOGISTIC REGRESSION ===\")\n",
    "print(\"Why Logistic Regression?\")\n",
    "print(\"- Simple and interpretable\")\n",
    "print(\"- Good baseline for binary classification\")\n",
    "print(\"- Provides probability estimates\")\n",
    "print(\"- Fast training\")\n",
    "\n",
    "# Initialize Logistic Regression\n",
    "lr_model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,  # Increase iterations for convergence\n",
    "    solver='liblinear'  # Good for small datasets\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "lr_results = train_model(lr_model, X_train, y_train, X_test, y_test, \"Logistic Regression\")\n",
    "\n",
    "# Show feature importance (coefficients)\n",
    "feature_importance_lr = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'coefficient': lr_model.coef_[0],\n",
    "    'abs_coefficient': np.abs(lr_model.coef_[0])\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features (Logistic Regression):\")\n",
    "print(feature_importance_lr.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af86a3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Model 2 - Random Forest\n",
    "print(\"\\n=== MODEL 2: RANDOM FOREST ===\")\n",
    "print(\"Why Random Forest?\")\n",
    "print(\"- Handles non-linear relationships\")\n",
    "print(\"- Robust to outliers\")\n",
    "print(\"- Provides feature importance\")\n",
    "print(\"- Good for mixed data types\")\n",
    "\n",
    "# Initialize Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,  # Number of trees\n",
    "    random_state=42,\n",
    "    max_depth=10,  # Prevent overfitting\n",
    "    min_samples_split=5,  # Minimum samples to split a node\n",
    "    min_samples_leaf=2,   # Minimum samples in leaf node\n",
    "    class_weight='balanced'  # Handle class imbalance\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_results = train_model(rf_model, X_train, y_train, X_test, y_test, \"Random Forest\")\n",
    "\n",
    "# Show feature importance\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features (Random Forest):\")\n",
    "print(feature_importance_rf.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f441efb9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Model 3 - K-Nearest Neighbors \n",
    "print(\"\\n=== MODEL 3: K-NEAREST NEIGHBORS ===\")\n",
    "print(\"Why KNN?\")\n",
    "print(\"- Simple and intuitive\")\n",
    "print(\"- Non-parametric (makes no assumptions)\")\n",
    "print(\"- Good for small datasets\")\n",
    "print(\"- Instance-based learning\")\n",
    "\n",
    "# We need to find optimal k value first\n",
    "print(\"\\nFinding optimal k value...\")\n",
    "\n",
    "# Test different k values\n",
    "k_values = range(1, 21)\n",
    "cv_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "# Find optimal k\n",
    "optimal_k = k_values[np.argmax(cv_scores)]\n",
    "print(f\"Optimal k value: {optimal_k}\")\n",
    "\n",
    "# Plot k vs accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, cv_scores, marker='o')\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Cross-Validation Accuracy')\n",
    "plt.title('K Value Optimization for KNN')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Train with optimal k\n",
    "knn_model = KNeighborsClassifier(\n",
    "    n_neighbors=optimal_k,\n",
    "    weights='distance',  # Weight by distance (closer neighbors matter more)\n",
    "    metric='euclidean'   # Distance metric\n",
    ")\n",
    "\n",
    "knn_results = train_model(knn_model, X_train, y_train, X_test, y_test, f\"KNN (k={optimal_k})\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
