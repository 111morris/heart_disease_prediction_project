{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages \n",
    "!pip install kaggle\n",
    "!pip install pandas numpy scikit-learn matplotlib seaborn\n",
    "\n",
    "# Import all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d35ade",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Direct download from UCI repository\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset directly from the corrected URL\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
    "# The dataset does not have a header row, and the columns are not named.\n",
    "# We need to provide column names manually based on the dataset description.\n",
    "column_names = [\n",
    "    'age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n",
    "    'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'\n",
    "]\n",
    "df = pd.read_csv(url, names=column_names, na_values=\"?\") # Handle missing values represented by '?'\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1778ef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('heart.csv') or you can use the variable from alternative method\n",
    "\n",
    "# 1. View first few rows\n",
    "print(\"=== First 5 rows of the dataset ===\")\n",
    "print(df.head())\n",
    "print(\"\\n\")\n",
    "\n",
    "# 2. Check dataset info (data types, non-null counts)\n",
    "print(\"=== Dataset Information ===\")\n",
    "print(df.info())\n",
    "print(\"\\n\")\n",
    "\n",
    "# 3. Summary statistics\n",
    "print(\"=== Summary Statistics ===\")\n",
    "print(df.describe())\n",
    "print(\"\\n\")\n",
    "\n",
    "# 4. Check for missing values\n",
    "print(\"=== Missing Values Check ===\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\n\")\n",
    "\n",
    "# 5. Check target variable distribution\n",
    "print(\"=== Target Variable Distribution ===\")\n",
    "print(df['target'].value_counts())\n",
    "print(f\"Percentage with heart disease: {(df['target'].sum()/len(df)*100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def6a0e1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check for missing values again after loading with '?' as NA\n",
    "print(\"=== Missing values after loading with na_values='?' ===\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Handle missing values: Fill missing values in 'ca' and 'thal' with the mode\n",
    "for col in ['ca', 'thal']:\n",
    "    if df[col].isnull().any():\n",
    "        mode_value = df[col].mode()[0]\n",
    "        df[col].fillna(mode_value, inplace=True)\n",
    "        print(f\"Filled missing values in '{col}' with the mode: {mode_value}\")\n",
    "\n",
    "print(\"\\n=== Missing values after handling ===\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edec764d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Converts columns to appropriate data types\n",
    "# 'ca' and 'thal' are currently float due to missing values, convert them to int\n",
    "for col in ['ca', 'thal', 'target']:\n",
    "    df[col] = df[col].astype(int)\n",
    "\n",
    "print(\"=== Data types after conversion ===\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fad11e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Explore the distribution of categorical features\n",
    "categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal', 'target']\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(categorical_features):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    df[col].value_counts().plot(kind='bar', color=sns.color_palette('viridis'))\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd1665",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(numerical_features):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    sns.histplot(df[col], kde=True, color=sns.color_palette('viridis')[i])\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72509489",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create a feature dictionary for reference\n",
    "feature_dict = {\n",
    "    'age': 'Age in years',\n",
    "    'sex': 'Sex (1 = male, 0 = female)',\n",
    "    'cp': 'Chest pain type (0-3)',\n",
    "    'trestbps': 'Resting blood pressure (mm Hg)',\n",
    "    'chol': 'Serum cholesterol (mg/dl)',\n",
    "    'fbs': 'Fasting blood sugar > 120 mg/dl (1 = true, 0 = false)',\n",
    "    'restecg': 'Resting ECG results (0-2)',\n",
    "    'thalach': 'Maximum heart rate achieved',\n",
    "    'exang': 'Exercise induced angina (1 = yes, 0 = no)',\n",
    "    'oldpeak': 'ST depression induced by exercise',\n",
    "    'slope': 'Slope of peak exercise ST segment (0-2)',\n",
    "    'ca': 'Number of major vessels colored by fluoroscopy (0-3)',\n",
    "    'thal': 'Thalassemia (0 = normal, 1 = fixed defect, 2 = reversable defect)',\n",
    "    'target': 'Heart disease presence (1 = yes, 0 = no)'\n",
    "}\n",
    "\n",
    "print(\"=== Feature Descriptions ===\")\n",
    "for feature, description in feature_dict.items():\n",
    "    print(f\"{feature}: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6166c2dc",
   "metadata": {},
   "source": [
    "A solution: install kaggle package first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a5f764",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade kaggle\n",
    "!kaggle datasets download -v -d ronitf/heart-disease-uci --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326aac7a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "for fn in uploaded.keys():\n",
    "    print(f'User uploaded file \"{fn}\" with length {len(uploaded[fn])} bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38eed90",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create a simple visualization to understand the data\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Subplot 1: Target distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "df['target'].value_counts().plot(kind='bar', color=['lightcoral', 'lightblue'])\n",
    "plt.title('Heart Disease Distribution')\n",
    "plt.xlabel('Target (0=No Disease, 1=Disease)')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Subplot 2: Age distribution by target\n",
    "plt.subplot(1, 2, 2)\n",
    "df[df['target']==0]['age'].hist(alpha=0.7, label='No Disease', color='lightblue')\n",
    "df[df['target']==1]['age'].hist(alpha=0.7, label='Disease', color='lightcoral')\n",
    "plt.title('Age Distribution by Heart Disease Status')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e22d178",
   "metadata": {},
   "source": [
    "Task 2 Starts: Data preprocessing => cleaning and preparing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb7d5b9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# data quality assessment\n",
    "print(\"=== DATA QUALITY ASSESSMENT ===\\n\")\n",
    "\n",
    "# 1. Check dataset shape\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of patients: {df.shape[0]}\")\n",
    "print(f\"Number of features: {df.shape[1] - 1} (excluding target)\")\n",
    "\n",
    "# 2. Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Duplicate rows: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    print(\"Removing duplicates...\")\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"New shape after removing duplicates: {df.shape}\")\n",
    "\n",
    "# 3. Check for missing values (detailed)\n",
    "print(\"\\n=== Missing Values Analysis ===\")\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "missing_table = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Percentage': missing_percent\n",
    "})\n",
    "print(missing_table[missing_table['Missing Count'] > 0])\n",
    "\n",
    "# 4. Check data types\n",
    "print(\"\\n=== Data Types Check ===\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a52e29",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=== FEATURE ANALYSIS ===\\n\")\n",
    "\n",
    "# Create a detailed feature analysis\n",
    "for column in df.columns:\n",
    "    if column != 'target':\n",
    "        print(f\"\\n--- {column.upper()} ---\")\n",
    "        print(f\"Data type: {df[column].dtype}\")\n",
    "        print(f\"Unique values: {df[column].nunique()}\")\n",
    "        print(f\"Min: {df[column].min()}, Max: {df[column].max()}\")\n",
    "        print(f\"Value counts:\\n{df[column].value_counts().head(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cae85f",
   "metadata": {},
   "source": [
    "# Even though UCI dataset typically has no missing values, \n",
    "# here's how to handle them if they exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b8e56d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def handle_missing_values(df):\n",
    "    \"\"\"Comprehensive missing value handling\"\"\"\n",
    "    print(\"=== HANDLING MISSING VALUES ===\")\n",
    "    \n",
    "    # Check for missing values again\n",
    "    missing = df.isnull().sum()\n",
    "    \n",
    "    if missing.sum() == 0:\n",
    "        print(\"No missing values found!\")\n",
    "        return df\n",
    "    \n",
    "    # Strategy depends on the feature type\n",
    "    for column in df.columns:\n",
    "        if df[column].isnull().sum() > 0:\n",
    "            print(f\"\\nHandling missing values in {column}:\")\n",
    "            \n",
    "            # For numerical features\n",
    "            if df[column].dtype in ['int64', 'float64']:\n",
    "                # Use median (more robust than mean)\n",
    "                median_value = df[column].median()\n",
    "                df[column].fillna(median_value, inplace=True)\n",
    "                print(f\"  - Filled {df[column].isnull().sum()} missing values with median: {median_value}\")\n",
    "            \n",
    "            # For categorical features\n",
    "            else:\n",
    "                # Use mode (most frequent value)\n",
    "                mode_value = df[column].mode()[0]\n",
    "                df[column].fillna(mode_value, inplace=True)\n",
    "                print(f\"  - Filled {df[column].isnull().sum()} missing values with mode: {mode_value}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function\n",
    "df = handle_missing_values(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ba2319",
   "metadata": {},
   "source": [
    "# Outlier detection and handling \n",
    "# Detect outliers using statistical methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82319dab",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def detect_outliers(df, features):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    print(\"=== OUTLIER DETECTION ===\")\n",
    "    \n",
    "    outlier_indices = []\n",
    "    \n",
    "    for feature in features:\n",
    "        if feature != 'target' and df[feature].dtype in ['int64', 'float64']:\n",
    "            # Calculate IQR\n",
    "            Q1 = df[feature].quantile(0.25)\n",
    "            Q3 = df[feature].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            # Define outlier bounds\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            # Find outliers\n",
    "            outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n",
    "            \n",
    "            if len(outliers) > 0:\n",
    "                print(f\"\\n{feature}:\")\n",
    "                print(f\"  - Lower bound: {lower_bound:.2f}\")\n",
    "                print(f\"  - Upper bound: {upper_bound:.2f}\")\n",
    "                print(f\"  - Outliers found: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)\")\n",
    "                \n",
    "                # Visualize outliers\n",
    "                plt.figure(figsize=(10, 4))\n",
    "                \n",
    "                plt.subplot(1, 2, 1)\n",
    "                df[feature].hist(bins=30, alpha=0.7)\n",
    "                plt.axvline(lower_bound, color='red', linestyle='--', label=f'Lower bound: {lower_bound:.1f}')\n",
    "                plt.axvline(upper_bound, color='red', linestyle='--', label=f'Upper bound: {upper_bound:.1f}')\n",
    "                plt.title(f'{feature} Distribution with Outlier Bounds')\n",
    "                plt.legend()\n",
    "                \n",
    "                plt.subplot(1, 2, 2)\n",
    "                df.boxplot(column=feature)\n",
    "                plt.title(f'{feature} Boxplot')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "    \n",
    "    return outlier_indices\n",
    "\n",
    "# Apply outlier detection\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "detect_outliers(df, numerical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61fc696",
   "metadata": {},
   "source": [
    "# Feature engineering and selection\n",
    "# Create new features that might be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ad4a9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"=== FEATURE ENGINEERING ===\")\n",
    "\n",
    "# 1. Age groups\n",
    "df['age_group'] = pd.cut(df['age'], \n",
    "                        bins=[0, 40, 50, 60, 70, 100], \n",
    "                        labels=['<40', '40-50', '50-60', '60-70', '70+'])\n",
    "\n",
    "# 2. Cholesterol categories (using medical standards)\n",
    "df['chol_category'] = pd.cut(df['chol'], \n",
    "                            bins=[0, 200, 240, 1000], \n",
    "                            labels=['Desirable', 'Borderline', 'High'])\n",
    "\n",
    "# 3. Blood pressure categories\n",
    "df['bp_category'] = pd.cut(df['trestbps'], \n",
    "                          bins=[0, 120, 130, 140, 180, 300], \n",
    "                          labels=['Normal', 'Elevated', 'Stage1', 'Stage2', 'Crisis'])\n",
    "\n",
    "# 4. Heart rate efficiency (thalach vs age)\n",
    "df['heart_rate_efficiency'] = df['thalach'] / df['age']\n",
    "\n",
    "print(\"New features created:\")\n",
    "print(\"- age_group: Categorized age ranges\")\n",
    "print(\"- chol_category: Cholesterol levels based on medical standards\")\n",
    "print(\"- bp_category: Blood pressure categories\")\n",
    "print(\"- heart_rate_efficiency: Ratio of max heart rate to age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59b67ab",
   "metadata": {},
   "source": [
    "# Data normalization and scalling\n",
    "# Separate features and target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc82e14",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "print(\"=== FEATURE SCALING ===\")\n",
    "\n",
    "# Identify numerical and categorical features\n",
    "numerical_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numerical features: {len(numerical_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")\n",
    "\n",
    "# Handle categorical features (if any)\n",
    "if categorical_features:\n",
    "    X = pd.get_dummies(X, columns=categorical_features, drop_first=True)\n",
    "    print(f\"After one-hot encoding: {X.shape[1]} features\")\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X.copy()\n",
    "X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "\n",
    "print(\"\\nScaling completed!\")\n",
    "print(f\"Original feature range example (age): {X['age'].min():.1f} to {X['age'].max():.1f}\")\n",
    "print(f\"Scaled feature range example (age): {X_scaled['age'].min():.2f} to {X_scaled['age'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e253d4ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"=== TRAIN-TEST SPLIT ===\")\n",
    "\n",
    "# Split the data (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Training set distribution: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Test set distribution: {y_test.value_counts().to_dict()}\")\n",
    "\n",
    "# Verify the split maintains the class distribution\n",
    "train_ratio = y_train.sum() / len(y_train)\n",
    "test_ratio = y_test.sum() / len(y_test)\n",
    "print(f\"\\nClass balance check:\")\n",
    "print(f\"Training set positive ratio: {train_ratio:.3f}\")\n",
    "print(f\"Test set positive ratio: {test_ratio:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ac65d0",
   "metadata": {},
   "source": [
    "# Final Data verification \n",
    "# Create a comprehensive verification report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c327ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=== FINAL VERIFICATION ===\\n\")\n",
    "\n",
    "def verify_preprocessing(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Verify that preprocessing was successful\"\"\"\n",
    "    \n",
    "    # 1. Check shapes\n",
    "    print(\"1. Shape Verification:\")\n",
    "    print(f\"   X_train: {X_train.shape}\")\n",
    "    print(f\"   X_test: {X_test.shape}\")\n",
    "    print(f\"   y_train: {y_train.shape}\")\n",
    "    print(f\"   y_test: {y_test.shape}\")\n",
    "    \n",
    "    # 2. Check for missing values\n",
    "    print(\"\\n2. Missing Values Check:\")\n",
    "    print(f\"   X_train missing: {X_train.isnull().sum().sum()}\")\n",
    "    print(f\"   X_test missing: {X_test.isnull().sum().sum()}\")\n",
    "    \n",
    "    # 3. Check scaling\n",
    "    print(\"\\n3. Scaling Verification (first 3 numerical features):\")\n",
    "    numerical_cols = X_train.select_dtypes(include=[np.number]).columns[:3]\n",
    "    for col in numerical_cols:\n",
    "        print(f\"   {col}: mean={X_train[col].mean():.3f}, std={X_train[col].std():.3f}\")\n",
    "    \n",
    "    # 4. Check target distribution\n",
    "    print(\"\\n4. Target Distribution:\")\n",
    "    print(f\"   Training: {y_train.value_counts().to_dict()}\")\n",
    "    print(f\"   Testing: {y_test.value_counts().to_dict()}\")\n",
    "    \n",
    "    print(\"\\nâœ… Preprocessing completed successfully!\")\n",
    "\n",
    "# Run verification\n",
    "verify_preprocessing(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b33fb1d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the scaler for later use\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Save the preprocessed data\n",
    "preprocessed_data = {\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'feature_names': X_train.columns.tolist()\n",
    "}\n",
    "\n",
    "with open('preprocessed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessed_data, f)\n",
    "\n",
    "print(\"Preprocessed data saved!\")\n",
    "print(\"Files created: scaler.pkl, preprocessed_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4123f5",
   "metadata": {},
   "source": [
    "# Summary Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64ef700",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the scaler for later use\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Save the preprocessed data\n",
    "preprocessed_data = {\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'feature_names': X_train.columns.tolist()\n",
    "}\n",
    "\n",
    "with open('preprocessed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessed_data, f)\n",
    "\n",
    "print(\"Preprocessed data saved!\")\n",
    "print(\"Files created: scaler.pkl, preprocessed_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc70f2b",
   "metadata": {},
   "source": [
    "# Check if one-hot encoding created too many features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a8746c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Original features: {X.shape[1]}\")\n",
    "print(f\"After preprocessing: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227b7a06",
   "metadata": {},
   "source": [
    "# Task 3: Model Building - Training Multiple Classification Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b7f48",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import additional libraries needed for modeling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "import time\n",
    "\n",
    "print(\"=== MODEL BUILDING SETUP ===\")\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "print(f\"Features: {list(X_train.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e41d7f",
   "metadata": {},
   "source": [
    "# create a model training framework \n",
    "# Create a function to train and evaluate models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcf1965",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Train a model and return performance metrics\n",
    "    \n",
    "    Parameters:\n",
    "    model: sklearn model instance\n",
    "    X_train: training features\n",
    "    y_train: training target\n",
    "    X_test: test features\n",
    "    y_test: test target\n",
    "    model_name: name of the model for display\n",
    "    \n",
    "    Returns:\n",
    "    Dictionary with model results\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Training {model_name} ===\")\n",
    "    \n",
    "    # Record training time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'model': model,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'training_time': training_time\n",
    "    }\n",
    "    \n",
    "    print(f\"Training completed in {training_time:.3f} seconds\")\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"Cross-validation: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a958ea",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Model 1 - Logistic Regression\n",
    "print(\"=== MODEL 1: LOGISTIC REGRESSION ===\")\n",
    "print(\"Why Logistic Regression?\")\n",
    "print(\"- Simple and interpretable\")\n",
    "print(\"- Good baseline for binary classification\")\n",
    "print(\"- Provides probability estimates\")\n",
    "print(\"- Fast training\")\n",
    "\n",
    "# Initialize Logistic Regression\n",
    "lr_model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,  # Increase iterations for convergence\n",
    "    solver='liblinear'  # Good for small datasets\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "lr_results = train_model(lr_model, X_train, y_train, X_test, y_test, \"Logistic Regression\")\n",
    "\n",
    "# Show feature importance (coefficients)\n",
    "feature_importance_lr = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'coefficient': lr_model.coef_[0],\n",
    "    'abs_coefficient': np.abs(lr_model.coef_[0])\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features (Logistic Regression):\")\n",
    "print(feature_importance_lr.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af86a3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Model 2 - Random Forest\n",
    "print(\"\\n=== MODEL 2: RANDOM FOREST ===\")\n",
    "print(\"Why Random Forest?\")\n",
    "print(\"- Handles non-linear relationships\")\n",
    "print(\"- Robust to outliers\")\n",
    "print(\"- Provides feature importance\")\n",
    "print(\"- Good for mixed data types\")\n",
    "\n",
    "# Initialize Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,  # Number of trees\n",
    "    random_state=42,\n",
    "    max_depth=10,  # Prevent overfitting\n",
    "    min_samples_split=5,  # Minimum samples to split a node\n",
    "    min_samples_leaf=2,   # Minimum samples in leaf node\n",
    "    class_weight='balanced'  # Handle class imbalance\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_results = train_model(rf_model, X_train, y_train, X_test, y_test, \"Random Forest\")\n",
    "\n",
    "# Show feature importance\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features (Random Forest):\")\n",
    "print(feature_importance_rf.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f441efb9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Model 3 - K-Nearest Neighbors \n",
    "print(\"\\n=== MODEL 3: K-NEAREST NEIGHBORS ===\")\n",
    "print(\"Why KNN?\")\n",
    "print(\"- Simple and intuitive\")\n",
    "print(\"- Non-parametric (makes no assumptions)\")\n",
    "print(\"- Good for small datasets\")\n",
    "print(\"- Instance-based learning\")\n",
    "\n",
    "# We need to find optimal k value first\n",
    "print(\"\\nFinding optimal k value...\")\n",
    "\n",
    "# Test different k values\n",
    "k_values = range(1, 21)\n",
    "cv_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "# Find optimal k\n",
    "optimal_k = k_values[np.argmax(cv_scores)]\n",
    "print(f\"Optimal k value: {optimal_k}\")\n",
    "\n",
    "# Plot k vs accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, cv_scores, marker='o')\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Cross-Validation Accuracy')\n",
    "plt.title('K Value Optimization for KNN')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Train with optimal k\n",
    "knn_model = KNeighborsClassifier(\n",
    "    n_neighbors=optimal_k,\n",
    "    weights='distance',  # Weight by distance (closer neighbors matter more)\n",
    "    metric='euclidean'   # Distance metric\n",
    ")\n",
    "\n",
    "knn_results = train_model(knn_model, X_train, y_train, X_test, y_test, f\"KNN (k={optimal_k})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7961aac4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Model 4 - Neural Network \n",
    "print(\"\\n=== MODEL 4: NEURAL NETWORK ===\")\n",
    "print(\"Why Neural Network?\")\n",
    "print(\"- Can learn complex patterns\")\n",
    "print(\"- Good for non-linear relationships\")\n",
    "print(\"- Scalable to large datasets\")\n",
    "\n",
    "# Import MLPClassifier (Multi-layer Perceptron)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Initialize Neural Network\n",
    "nn_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),  # Two hidden layers\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    early_stopping=True,  # Prevent overfitting\n",
    "    validation_fraction=0.1,\n",
    "    alpha=0.01,  # L2 regularization\n",
    "    learning_rate_init=0.001\n",
    ")\n",
    "\n",
    "print(\"Training Neural Network (this may take a moment)...\")\n",
    "nn_results = train_model(nn_model, X_train, y_train, X_test, y_test, \"Neural Network\")\n",
    "\n",
    "# Show training loss curve\n",
    "if hasattr(nn_model, 'loss_curve_'):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(nn_model.loss_curve_)\n",
    "    plt.title('Neural Network Training Loss')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7664582",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "all_results = [lr_results, rf_results, knn_results, nn_results]\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': [result['model_name'] for result in all_results],\n",
    "    'Accuracy': [result['accuracy'] for result in all_results],\n",
    "    'Precision': [result['precision'] for result in all_results],\n",
    "    'Recall': [result['recall'] for result in all_results],\n",
    "    'F1-Score': [result['f1_score'] for result in all_results],\n",
    "    'CV Mean': [result['cv_mean'] for result in all_results],\n",
    "    'CV Std': [result['cv_std'] for result in all_results],\n",
    "    'Training Time (s)': [result['training_time'] for result in all_results]\n",
    "})\n",
    "\n",
    "print(\"=== MODEL COMPARISON ===\")\n",
    "print(comparison_df.round(3))\n",
    "\n",
    "# Find best model\n",
    "best_model_idx = np.argmax([result['f1_score'] for result in all_results])\n",
    "best_model = all_results[best_model_idx]\n",
    "print(f\"\\nBest Model: {best_model['model_name']}\")\n",
    "print(f\"F1-Score: {best_model['f1_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1856c05",
   "metadata": {},
   "source": [
    "# Improving the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6330ea4b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# improve the best model with hyperparameter tuning\n",
    "print(f\"\\n=== HYPERPARAMETER TUNING FOR {best_model['model_name']} ===\")\n",
    "\n",
    "# Define parameter grids for each model type\n",
    "if 'Logistic Regression' in best_model['model_name']:\n",
    "    param_grid = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear']\n",
    "    }\n",
    "elif 'Random Forest' in best_model['model_name']:\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [5, 10, 15, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "elif 'KNN' in best_model['model_name']:\n",
    "    param_grid = {\n",
    "        'n_neighbors': range(3, 20, 2),\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "    }\n",
    "\n",
    "# Perform Grid Search (using a smaller parameter set for speed)\n",
    "print(\"Performing grid search...\")\n",
    "grid_search = GridSearchCV(\n",
    "    best_model['model'].__class__(),\n",
    "    param_grid,\n",
    "    cv=3,  # 3-fold CV for speed\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "# Train final model with best parameters\n",
    "final_model = grid_search.best_estimator_\n",
    "final_results = train_model(final_model, X_train, y_train, X_test, y_test, \n",
    "                           f\"Tuned {best_model['model_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64583eaa",
   "metadata": {},
   "source": [
    "# Save the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa17e2aa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Save the best performing model\n",
    "import joblib\n",
    "\n",
    "# Determine which model to save\n",
    "if final_results['f1_score'] > best_model['f1_score']:\n",
    "    model_to_save = final_model\n",
    "    model_name = f\"tuned_{best_model['model_name'].lower().replace(' ', '_')}\"\n",
    "else:\n",
    "    model_to_save = best_model['model']\n",
    "    model_name = best_model['model_name'].lower().replace(' ', '_')\n",
    "\n",
    "# Save model\n",
    "model_filename = f'best_heart_disease_model_{model_name}.pkl'\n",
    "joblib.dump(model_to_save, model_filename)\n",
    "\n",
    "# Save scaler (we'll need it for new predictions)\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "print(f\"\\nðŸ’¾ Model saved as: {model_filename}\")\n",
    "print(f\"ðŸ’¾ Scaler saved as: scaler.pkl\")\n",
    "\n",
    "# Create a model summary\n",
    "model_summary = {\n",
    "    'best_model': model_name,\n",
    "    'accuracy': max(final_results['accuracy'], best_model['accuracy']),\n",
    "    'precision': max(final_results['precision'], best_model['precision']),\n",
    "    'recall': max(final_results['recall'], best_model['recall']),\n",
    "    'f1_score': max(final_results['f1_score'], best_model['f1_score']),\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'features_used': list(X_train.columns)\n",
    "}\n",
    "\n",
    "print(\"\\n=== FINAL MODEL SUMMARY ===\")\n",
    "for key, value in model_summary.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ce998d",
   "metadata": {},
   "source": [
    "# Create comprehensive feature importance visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df537df7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Get feature importance from the best model\n",
    "if hasattr(model_to_save, 'feature_importances_'):  # Tree-based models\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': model_to_save.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    # Plot top 15 features\n",
    "    plt.subplot(2, 2, 1)\n",
    "    top_features = importance_df.head(15)\n",
    "    plt.barh(top_features['feature'], top_features['importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title(f'Top 15 Features - {model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "elif hasattr(model_to_save, 'coef_'):  # Linear models\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'coefficient': model_to_save.coef_[0],\n",
    "        'abs_coefficient': np.abs(model_to_save.coef_[0])\n",
    "    }).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "    # Plot top 15 features\n",
    "    plt.subplot(2, 2, 1)\n",
    "    top_features = importance_df.head(15)\n",
    "    colors = ['red' if x < 0 else 'blue' for x in top_features['coefficient']]\n",
    "    plt.barh(top_features['feature'], top_features['coefficient'], color=colors)\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.title(f'Top 15 Features - {model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "# Plot model comparison\n",
    "plt.subplot(2, 2, 2)\n",
    "models = [result['model_name'] for result in all_results]\n",
    "accuracies = [result['accuracy'] for result in all_results]\n",
    "plt.bar(models, accuracies, color=['skyblue', 'lightgreen', 'orange', 'pink'])\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Plot F1-score comparison\n",
    "plt.subplot(2, 2, 3)\n",
    "f1_scores = [result['f1_score'] for result in all_results]\n",
    "plt.bar(models, f1_scores, color=['skyblue', 'lightgreen', 'orange', 'pink'])\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.title('Model F1-Score Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Plot training time comparison\n",
    "plt.subplot(2, 2, 4)\n",
    "training_times = [result['training_time'] for result in all_results]\n",
    "plt.bar(models, training_times, color=['skyblue', 'lightgreen', 'orange', 'pink'])\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Training Time (seconds)')\n",
    "plt.title('Training Time Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68eff77",
   "metadata": {},
   "source": [
    "# visualization for logistic regression and neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ff4089",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# For Logistic Regression\n",
    "LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "\n",
    "# For Neural Network\n",
    "MLPClassifier(max_iter=2000, early_stopping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dee4427",
   "metadata": {},
   "source": [
    "# visualization for random forest - reduce complexity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0a0e6e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Random Forest - reduce complexity\n",
    "RandomForestClassifier(max_depth=10, min_samples_split=5, min_samples_leaf=2)\n",
    "\n",
    "# Add regularization\n",
    "LogisticRegression(C=0.1, penalty='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229e15db",
   "metadata": {},
   "source": [
    "# Use class_weight parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308cc27d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Use class_weight parameter\n",
    "RandomForestClassifier(class_weight='balanced')\n",
    "\n",
    "# Or use SMOTE for oversampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdc6394",
   "metadata": {},
   "source": [
    "# Model evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb226723",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import all necessary evaluation libraries\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                           f1_score, confusion_matrix, classification_report,\n",
    "                           roc_curve, auc, precision_recall_curve)\n",
    "from sklearn.metrics import matthews_corrcoef, cohen_kappa_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=== MODEL EVALUATION SETUP ===\")\n",
    "print(\"Models to evaluate:\")\n",
    "for i, result in enumerate(all_results):\n",
    "    print(f\"{i+1}. {result['model_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b345dd61",
   "metadata": {},
   "source": [
    "# metrics calculation for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0585c76",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_comprehensive_metrics(y_true, y_pred, y_pred_proba, model_name):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive evaluation metrics\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with all metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== {model_name} - Detailed Metrics ===\")\n",
    "\n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    # Advanced metrics\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)  # Matthews Correlation Coefficient\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)  # Cohen's Kappa\n",
    "\n",
    "    # Classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['No Disease', 'Disease']))\n",
    "\n",
    "    metrics_dict = {\n",
    "        'model_name': model_name,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'mcc': mcc,\n",
    "        'kappa': kappa,\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "# Calculate metrics for all models\n",
    "all_metrics = []\n",
    "for result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38561075",
   "metadata": {},
   "source": [
    "# confusion matrix interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52b2784",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(all_metrics):\n",
    "    \"\"\"\n",
    "    Create confusion matrices for all models\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i, metrics in enumerate(all_metrics):\n",
    "        cm = confusion_matrix(metrics['y_true'], metrics['y_pred'])\n",
    "\n",
    "        # Create heatmap\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=['No Disease', 'Disease'],\n",
    "                   yticklabels=['No Disease', 'Disease'],\n",
    "                   ax=axes[i])\n",
    "\n",
    "        axes[i].set_title(f'{metrics[\"model_name\"]}\\n'\n",
    "                         f'Accuracy: {metrics[\"accuracy\"]:.3f}')\n",
    "        axes[i].set_xlabel('Predicted')\n",
    "        axes[i].set_ylabel('Actual')\n",
    "\n",
    "        # Add percentage annotations\n",
    "        total = cm.sum()\n",
    "        for j in range(2):\n",
    "            for k in range(2):\n",
    "                percentage = cm[j, k] / total * 100\n",
    "                axes[i].text(k + 0.5, j + 0.7, f'({percentage:.1f}%)',\n",
    "                           ha='center', va='center', fontsize=10, color='red')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print confusion matrix interpretation\n",
    "    print(\"\\n=== CONFUSION MATRIX INTERPRETATION ===\")\n",
    "    for metrics in all_metrics:\n",
    "        cm = confusion_matrix(metrics['y_true'], metrics['y_pred'])\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "        print(f\"\\n{metrics['model_name']}:\")\n",
    "        print(f\"  True Negatives (Correctly predicted no disease): {tn}\")\n",
    "        print(f\"  False Positives (Incorrectly predicted disease): {fp}\")\n",
    "        print(f\"  False Negatives (Missed disease cases): {fn}\")\n",
    "        print(f\"  True Positives (Correctly predicted disease): {tp}\")\n",
    "        print(f\"  Sensitivity (Recall): {tp/(tp+fn):.3f}\")\n",
    "        print(f\"  Specificity: {tn/(tn+fp):.3f}\")\n",
    "\n",
    "plot_confusion_matrices(all_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a75202",
   "metadata": {},
   "source": [
    "# ROC Curve Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0276fac0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ROC Curve Analysis\n",
    "def plot_roc_curves(all_metrics):\n",
    "    \"\"\"\n",
    "    Plot ROC curves for all models\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    for metrics in all_metrics:\n",
    "        fpr, tpr, _ = roc_curve(metrics['y_true'], metrics['y_pred_proba'])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        plt.plot(fpr, tpr,\n",
    "                label=f\"{metrics['model_name']} (AUC = {roc_auc:.3f})\",\n",
    "                linewidth=2)\n",
    "\n",
    "    # Plot diagonal line\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "    plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "    plt.title('ROC Curves Comparison - Heart Disease Prediction')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    # Print AUC interpretation\n",
    "    print(\"\\n=== ROC-AUC INTERPRETATION ===\")\n",
    "    print(\"AUC (Area Under Curve) Interpretation:\")\n",
    "    print(\"- 1.0 = Perfect classifier\")\n",
    "    print(\"- 0.9-1.0 = Excellent\")\n",
    "    print(\"- 0.8-0.9 = Good\")\n",
    "    print(\"- 0.7-0.8 = Fair\")\n",
    "    print(\"- 0.6-0.7 = Poor\")\n",
    "    print(\"- 0.5 = Random guessing\")\n",
    "\n",
    "    for metrics in all_metrics:\n",
    "        fpr, tpr, _ = roc_curve(metrics['y_true'], metrics['y_pred_proba'])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        print(f\"\\n{metrics['model_name']}: AUC = {roc_auc:.3f}\")\n",
    "\n",
    "plot_roc_curves(all_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27fdd14",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Precision-Recall Curve Analysis\n",
    "def plot_precision_recall_curves(all_metrics):\n",
    "    \"\"\"\n",
    "    Plot Precision-Recall curves for all models\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    for metrics in all_metrics:\n",
    "        precision, recall, _ = precision_recall_curve(metrics['y_true'],\n",
    "                                                     metrics['y_pred_proba'])\n",
    "\n",
    "        # Calculate Average Precision\n",
    "        avg_precision = np.mean(precision)\n",
    "\n",
    "        plt.plot(recall, precision,\n",
    "                label=f\"{metrics['model_name']} (AP = {avg_precision:.3f})\",\n",
    "                linewidth=2)\n",
    "\n",
    "    # Add baseline (random classifier performance)\n",
    "    baseline = np.sum(metrics['y_true']) / len(metrics['y_true'])\n",
    "    plt.axhline(y=baseline, color='k', linestyle='--',\n",
    "               label=f'Baseline (Random): {baseline:.3f}')\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall (Sensitivity)')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curves - Heart Disease Prediction')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    # Print interpretation\n",
    "    print(\"\\n=== PRECISION-RECALL INTERPRETATION ===\")\n",
    "    print(\"For Heart Disease Prediction:\")\n",
    "    print(\"- High Precision = Few false alarms (healthy predicted as diseased)\")\n",
    "    print(\"- High Recall = Few missed cases (disease not detected)\")\n",
    "    print(\"- Average Precision = Summarizes the curve as a single number\")\n",
    "\n",
    "    # Calculate and display specific metrics\n",
    "    for metrics in all_metrics:\n",
    "        precision, recall, thresholds = precision_recall_curve(metrics['y_true'],\n",
    "                                                              metrics['y_pred_proba'])\n",
    "        avg_precision = np.mean(precision)\n",
    "        print(f\"\\n{metrics['model_name']}:\")\n",
    "        print(f\"  Average Precision: {avg_precision:.3f}\")\n",
    "        print(f\"  Precision: {metrics['precision']:.3f}\")\n",
    "        print(f\"  Recall: {metrics['recall']:.3f}\")\n",
    "\n",
    "plot_precision_recall_curves(all_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3253af4",
   "metadata": {},
   "source": [
    "# Advanced Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804d482c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Advanced Metrics Comparison\n",
    "def create_metrics_comparison_table(all_metrics):\n",
    "    \"\"\"\n",
    "    Create comprehensive metrics comparison table\n",
    "    \"\"\"\n",
    "    # Create detailed comparison dataframe\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Model': [m['model_name'] for m in all_metrics],\n",
    "        'Accuracy': [m['accuracy'] for m in all_metrics],\n",
    "        'Precision': [m['precision'] for m in all_metrics],\n",
    "        'Recall': [m['recall'] for m in all_metrics],\n",
    "        'F1-Score': [m['f1_score'] for m in all_metrics],\n",
    "        'MCC': [m['mcc'] for m in all_metrics],\n",
    "        'Kappa': [m['kappa'] for m in all_metrics]\n",
    "    })\n",
    "\n",
    "    # Add AUC scores\n",
    "    auc_scores = []\n",
    "    for metrics in all_metrics:\n",
    "        fpr, tpr, _ = roc_curve(metrics['y_true'], metrics['y_pred_proba'])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        auc_scores.append(roc_auc)\n",
    "\n",
    "    metrics_df['AUC'] = auc_scores\n",
    "\n",
    "    # Round to 3 decimal places\n",
    "    metrics_df = metrics_df.round(3)\n",
    "\n",
    "    print(\"=== COMPREHENSIVE METRICS COMPARISON ===\")\n",
    "    print(metrics_df.to_string(index=False))\n",
    "\n",
    "    # Highlight best values\n",
    "    print(\"\\n=== BEST PERFORMING MODELS ===\")\n",
    "    for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC', 'MCC']:\n",
    "        best_idx = metrics_df[metric].idxmax()\n",
    "        best_model = metrics_df.loc[best_idx, 'Model']\n",
    "        best_score = metrics_df.loc[best_idx, metric]\n",
    "        print(f\"{metric}: {best_model} ({best_score:.3f})\")\n",
    "\n",
    "    return metrics_df\n",
    "\n",
    "metrics_df = create_metrics_comparison_table(all_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f6063b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def interpret_medical_context(all_metrics):\n",
    "    \"\"\"\n",
    "    Interpret results in medical context\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MEDICAL CONTEXT INTERPRETATION\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for metrics in all_metrics:\n",
    "        cm = confusion_matrix(metrics['y_true'], metrics['y_pred'])\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "        print(f\"\\n{metrics['model_name']}:\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # Calculate medical metrics\n",
    "        sensitivity = tp / (tp + fn)  # True Positive Rate\n",
    "        specificity = tn / (tn + fp)  # True Negative Rate\n",
    "        ppv = tp / (tp + fp) if (tp + fp) > 0 else 0  # Positive Predictive Value\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # Negative Predictive Value\n",
    "\n",
    "        print(f\"Sensitivity (Recall): {sensitivity:.3f}\")\n",
    "        print(f\"  â†’ Ability to correctly identify patients WITH heart disease\")\n",
    "        print(f\"  â†’ {sensitivity*100:.1f}% of diseased patients were correctly identified\")\n",
    "\n",
    "        print(f\"\\nSpecificity: {specificity:.3f}\")\n",
    "        print(f\"  â†’ Ability to correctly identify patients WITHOUT heart disease\")\n",
    "        print(f\"  â†’ {specificity*100:.1f}% of healthy patients were correctly identified\")\n",
    "\n",
    "        print(f\"\\nPositive Predictive Value: {ppv:.3f}\")\n",
    "        print(f\"  â†’ When model predicts disease, it's correct {ppv*100:.1f}% of the time\")\n",
    "\n",
    "        print(f\"\\nNegative Predictive Value: {npv:.3f}\")\n",
    "        print(f\"  â†’ When model predicts no disease, it's correct {npv*100:.1f}% of the time\")\n",
    "\n",
    "        # Clinical implications\n",
    "        print(f\"\\nClinical Implications:\")\n",
    "        if sensitivity > 0.85:\n",
    "            print(\"   EXCELLENT: High sensitivity - few missed cases\")\n",
    "        elif sensitivity > 0.75:\n",
    "            print(\"   GOOD: Moderate sensitivity - some missed cases\")\n",
    "        else:\n",
    "            print(\"   POOR: Low sensitivity - many missed cases\")\n",
    "\n",
    "        if specificity > 0.85:\n",
    "            print(\"   EXCELLENT: High specificity - few false alarms\")\n",
    "        elif specificity > 0.75:\n",
    "            print(\"   GOOD: Moderate specificity - some false alarms\")\n",
    "        else:\n",
    "            print(\"   POOR: Low specificity - many false alarms\")\n",
    "\n",
    "interpret_medical_context(all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7c043b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Statistical Significance Testing\n",
    "from scipy import stats\n",
    "\n",
    "def perform_statistical_tests(all_metrics):\n",
    "    \"\"\"\n",
    "    Perform statistical significance tests\n",
    "    \"\"\"\n",
    "    print(\"\\n=== STATISTICAL SIGNIFICANCE TESTING ===\")\n",
    "\n",
    "    # McNemar's test for comparing two models\n",
    "    def mcnemar_test(y_true, model1_pred, model2_pred, model1_name, model2_name):\n",
    "        from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "        # Create contingency table\n",
    "        table = pd.crosstab(model1_pred == y_true, model2_pred == y_true)\n",
    "\n",
    "        if table.shape == (2, 2):\n",
    "            result = mcnemar(table, exact=True)\n",
    "            print(f\"\\nMcNemar's Test: {model1_name} vs {model2_name}\")\n",
    "            print(f\"  p-value: {result.pvalue:.4f}\")\n",
    "            if result.pvalue < 0.05:\n",
    "                print(\"  â†’ Statistically significant difference\")\n",
    "            else:\n",
    "                print(\"  â†’ No statistically significant difference\")\n",
    "\n",
    "    # Compare best model with others\n",
    "    best_metric = max(all_metrics, key=lambda x: x['f1_score'])\n",
    "    print(f\"\\nBest model: {best_metric['model_name']}\")\n",
    "\n",
    "    for metrics in all_metrics:\n",
    "        if metrics['model_name'] != best_metric['model_name']:\n",
    "            mcnemar_test(\n",
    "                metrics['y_true'],\n",
    "                best_metric['y_pred'],\n",
    "                metrics['y_pred'],\n",
    "                best_metric['model_name'],\n",
    "                metrics['model_name']\n",
    "            )\n",
    "\n",
    "perform_statistical_tests(all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2b101c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Error Analysis\n",
    "def detailed_error_analysis(best_metrics):\n",
    "    \"\"\"\n",
    "    Analyze prediction errors in detail\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== DETAILED ERROR ANALYSIS FOR {best_metrics['model_name']} ===\")\n",
    "\n",
    "    # Find misclassified cases\n",
    "    misclassified = X_test.copy()\n",
    "    misclassified['actual'] = best_metrics['y_true']\n",
    "    misclassified['predicted'] = best_metrics['y_pred']\n",
    "    misclassified['correct'] = best_metrics['y_true'] == best_metrics['y_pred']\n",
    "    misclassified = misclassified[~misclassified['correct']]\n",
    "\n",
    "    print(f\"Total misclassified cases: {len(misclassified)}\")\n",
    "\n",
    "    # Analyze false positives vs false negatives\n",
    "    false_positives = misclassified[misclassified['actual'] == 0]\n",
    "    false_negatives = misclassified[misclassified['actual'] == 1]\n",
    "\n",
    "    print(f\"False Positives (healthy predicted as diseased): {len(false_positives)}\")\n",
    "    print(f\"False Negatives (diseased predicted as healthy): {len(false_negatives)}\")\n",
    "\n",
    "    # Analyze characteristics of misclassified cases\n",
    "    if len(false_positives) > 0:\n",
    "        print(\"\\nFalse Positive Characteristics:\")\n",
    "        print(\"Average age:\", false_positives['age'].mean())\n",
    "        print(\"Average cholesterol:\", false_positives['chol'].mean())\n",
    "        print(\"Average resting BP:\", false_positives['trestbps'].mean())\n",
    "\n",
    "    if len(false_negatives) > 0:\n",
    "        print(\"\\nFalse Negative Characteristics:\")\n",
    "        print(\"Average age:\", false_negatives['age'].mean())\n",
    "        print(\"Average cholesterol:\", false_negatives['chol'].mean())\n",
    "        print(\"Average resting BP:\", false_negatives['trestbps'].mean())\n",
    "\n",
    "    # Create error analysis visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # Age distribution of errors\n",
    "    axes[0, 0].hist(false_positives['age'], alpha=0.7, label='False Positives', bins=10)\n",
    "    axes[0, 0].hist(false_negatives['age'], alpha=0.7, label='False Negatives', bins=10)\n",
    "    axes[0, 0].set_xlabel('Age')\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "    axes[0, 0].set_title('Age Distribution of Misclassified Cases')\n",
    "    axes[0, 0].legend()\n",
    "\n",
    "    # Cholesterol distribution of errors\n",
    "    axes[0, 1].hist(false_positives['chol'], alpha=0.7, label='False Positives', bins=10)\n",
    "    axes[0, 1].hist(false_negatives['chol'], alpha=0.7, label='False Negatives', bins=10)\n",
    "    axes[0, 1].set_xlabel('Cholesterol')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "    axes[0, 1].set_title('Cholesterol Distribution of Misclassified Cases')\n",
    "    axes[0, 1].legend()\n",
    "\n",
    "    # Error types pie chart\n",
    "    error_counts = [len(false_positives), len(false_negatives)]\n",
    "    error_labels = ['False Positives', 'False Negatives']\n",
    "    axes[1, 0].pie(error_counts, labels=error_labels, autopct='%1.1f%%',\n",
    "                   colors=['lightcoral', 'lightblue'])\n",
    "    axes[1, 0].set_title('Distribution of Error Types')\n",
    "\n",
    "    # Feature comparison between correct and incorrect predictions\n",
    "    correct_predictions = X_test[best_metrics['y_true'] == best_metrics['y_pred']]\n",
    "    incorrect_predictions = X_test[best_metrics['y_true'] != best_metrics['y_pred']]\n",
    "\n",
    "    feature_to_plot = 'age'  # You can change this to any feature\n",
    "    axes[1, 1].hist(correct_predictions[feature_to_plot], alpha=0.7,\n",
    "                    label='Correct Predictions', bins=15, density=True)\n",
    "    axes[1, 1].hist(incorrect_predictions[feature_to_plot], alpha=0.7,\n",
    "                    label='Incorrect Predictions', bins=15, density=True)\n",
    "    axes[1, 1].set_xlabel(feature_to_plot.title())\n",
    "    axes[1, 1].set_ylabel('Density')\n",
    "    axes[1, 1].set_title(f'{feature_to_plot.title()} Distribution: Correct vs Incorrect')\n",
    "    axes[1, 1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run error analysis for the best model\n",
    "best_metrics = max(all_metrics, key=lambda x: x['f1_score'])\n",
    "detailed_error_analysis(best_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5835f85",
   "metadata": {},
   "source": [
    "## creating final report and save in a report.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f086b1e3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_evaluation_report(all_metrics, model_name):\n",
    "    \"\"\"\n",
    "    Create a comprehensive evaluation report\n",
    "    \"\"\"\n",
    "    report = f\"\"\"\n",
    "    HEART DISEASE PREDICTION MODEL - EVALUATION REPORT\n",
    "    ===================================================\n",
    "    \n",
    "    Model: {model_name}\n",
    "    Evaluation Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}\n",
    "    \n",
    "    1. OVERALL PERFORMANCE\n",
    "    ----------------------\n",
    "    Accuracy: {max(all_metrics, key=lambda x: x['accuracy'])['accuracy']:.3f}\n",
    "    Precision: {max(all_metrics, key=lambda x: x['precision'])['precision']:.3f}\n",
    "    Recall: {max(all_metrics, key=lambda x: x['recall'])['recall']:.3f}\n",
    "    F1-Score: {max(all_metrics, key=lambda x: x['f1_score'])['f1_score']:.3f}\n",
    "    AUC-ROC: {max(all_metrics, key=lambda x: x.get('auc', 0)):.3f}\n",
    "    \n",
    "    2. CLINICAL PERFORMANCE\n",
    "    -----------------------\n",
    "    Sensitivity (Disease Detection Rate): {max(all_metrics, key=lambda x: x['recall'])['recall']:.3f}\n",
    "    Specificity (Healthy Identification Rate): {max(all_metrics, key=lambda x: x.get('specificity', 0)):.3f}\n",
    "    \n",
    "    3. MODEL COMPARISON\n",
    "    -------------------\n",
    "    Best Overall Model: {max(all_metrics, key=lambda x: x['f1_score'])['model_name']}\n",
    "    Most Accurate Model: {max(all_metrics, key=lambda x: x['accuracy'])['model_name']}\n",
    "    Most Sensitive Model: {max(all_metrics, key=lambda x: x['recall'])['model_name']}\n",
    "    \n",
    "    4. CLINICAL INTERPRETATION\n",
    "    --------------------------\n",
    "    - The model can correctly identify {max(all_metrics, key=lambda x: x['recall'])['recall']*100:.1f}% of patients with heart disease\n",
    "    - The model has a precision of {max(all_metrics, key=lambda x: x['precision'])['precision']*100:.1f}% when predicting heart disease\n",
    "    - Out of every 100 positive predictions, {max(all_metrics, key=lambda x: x['precision'])['precision']*100:.1f} are actually correct\n",
    "    \n",
    "    5. RECOMMENDATIONS\n",
    "    ------------------\n",
    "    - Model is {'suitable' if max(all_metrics, key=lambda x: x['f1_score'])['f1_score'] > 0.8 else 'needs improvement'} for clinical deployment\n",
    "    - Consider {'further tuning' if max(all_metrics, key=lambda x: x['f1_score'])['f1_score'] < 0.85 else 'validation on external dataset'}\n",
    "    \"\"\"\n",
    "    \n",
    "    print(report)\n",
    "    \n",
    "    # Save report to file\n",
    "    with open('model_evaluation_report.txt', 'w') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    print(\"\\n   Report saved as 'model_evaluation_report.txt'\")\n",
    "\n",
    "# Create final report\n",
    "best_model_name = max(all_metrics, key=lambda x: x['f1_score'])['model_name']\n",
    "create_evaluation_report(all_metrics, best_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507e9e81",
   "metadata": {},
   "source": [
    "# Common evaluation issues and solutions\n",
    "## issue 1: \"My model shows perfect performance\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83b10ee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check for data leakage\n",
    "print(\"Check if you're leaking target information:\")\n",
    "print(\"X_train columns:\", X_train.columns)\n",
    "print(\"Make sure target variable is not in features\")\n",
    "\n",
    "# Check train-test split\n",
    "print(\"Training accuracy:\", model.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", model.score(X_test, y_test))\n",
    "# If training >> test, you might be overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8960192d",
   "metadata": {},
   "source": [
    "## issue 2: \"Metrics seems too low\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d6de11",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "print(\"Class distribution in test set:\")\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "# Try different metrics\n",
    "print(\"Balanced accuracy:\", balanced_accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac927e0",
   "metadata": {},
   "source": [
    "## issue 3: \"Confusion matrix doesn't make sense\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16cb785",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Verify prediction format\n",
    "print(\"Unique predictions:\", np.unique(y_pred))\n",
    "print(\"Unique actual values:\", np.unique(y_test))\n",
    "print(\"Prediction shape:\", y_pred.shape)\n",
    "print(\"Actual shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815be600",
   "metadata": {},
   "source": [
    "# Visualization and results\n",
    "## Setting up a professional visualization style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ead6a82",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Set up professional plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Define consistent colors\n",
    "COLORS = {\n",
    "    'primary': '#2E86AB',\n",
    "    'secondary': '#A23B72',\n",
    "    'success': '#F18F01',\n",
    "    'danger': '#C73E1D',\n",
    "    'neutral': '#8B8B8B',\n",
    "    'background': '#F5F5F5'\n",
    "}\n",
    "\n",
    "# Set default figure size\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "\n",
    "print(\"=== PROFESSIONAL VISUALIZATION SETUP ===\")\n",
    "print(\"Color palette and style configured for professional presentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd2f6be",
   "metadata": {},
   "source": [
    "## Feature importance visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6b18c0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_feature_importance_chart():\n",
    "    \"\"\"\n",
    "    Create stunning feature importance chart\n",
    "    \"\"\"\n",
    "    # Get feature importance from best model (Random Forest)\n",
    "    best_rf_model = rf_results['model']\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': best_rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=True)\n",
    "    \n",
    "    # Take top 10 features\n",
    "    top_features = feature_importance.tail(10)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create horizontal bar chart\n",
    "    bars = plt.barh(top_features['feature'], top_features['importance'], \n",
    "                    color=plt.cm.viridis(np.linspace(0, 1, len(top_features))))\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel('Feature Importance Score', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Health Metrics', fontsize=14, fontweight='bold')\n",
    "    plt.title('Top 10 Most Important Factors for Heart Disease Prediction\\n'\n",
    "              '(Based on Random Forest Analysis)', fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (feature, importance) in enumerate(zip(top_features['feature'], \n",
    "                                                   top_features['importance'])):\n",
    "        plt.text(importance + 0.001, i, f'{importance:.3f}', \n",
    "                va='center', fontweight='bold')\n",
    "    \n",
    "    # Add feature descriptions\n",
    "    feature_descriptions = {\n",
    "        'cp': 'Chest Pain Type',\n",
    "        'thalach': 'Max Heart Rate',\n",
    "        'ca': 'Major Vessels',\n",
    "        'oldpeak': 'ST Depression',\n",
    "        'slope': 'ST Slope',\n",
    "        'exang': 'Exercise Angina',\n",
    "        'age': 'Age',\n",
    "        'thal': 'Thalassemia',\n",
    "        'trestbps': 'Resting BP',\n",
    "        'chol': 'Cholesterol'\n",
    "    }\n",
    "    \n",
    "    # Update y-axis labels with descriptions\n",
    "    new_labels = []\n",
    "    for feature in top_features['feature']:\n",
    "        if feature in feature_descriptions:\n",
    "            new_labels.append(f\"{feature_descriptions[feature]}\\n({feature})\")\n",
    "        else:\n",
    "            new_labels.append(feature)\n",
    "    \n",
    "    plt.gca().set_yticklabels(new_labels)\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Tight layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save high-resolution image\n",
    "    plt.savefig('feature_importance_chart.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"     Feature importance chart saved as 'feature_importance_chart.png'\")\n",
    "    \n",
    "    return top_features\n",
    "\n",
    "top_features = create_feature_importance_chart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4006b350",
   "metadata": {},
   "source": [
    "## Confusion Matrix Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f88371",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_confusion_matrix_heatmap():\n",
    "    \"\"\"\n",
    "    Create professional confusion matrix heatmap\n",
    "    \"\"\"\n",
    "    # Get best model results\n",
    "    best_model_results = max(all_results, key=lambda x: x['f1_score'])\n",
    "    cm = confusion_matrix(y_test, best_model_results['predictions'])\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Create heatmap with custom styling\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['No Heart Disease', 'Heart Disease'],\n",
    "                yticklabels=['No Heart Disease', 'Heart Disease'],\n",
    "                cbar_kws={'label': 'Number of Patients'})\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title(f'Confusion Matrix - {best_model_results[\"model_name\"]}\\n'\n",
    "              f'Accuracy: {best_model_results[\"accuracy\"]:.1%}', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Predicted Condition', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Actual Condition', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add percentage annotations\n",
    "    total = cm.sum()\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            percentage = cm[i, j] / total * 100\n",
    "            plt.text(j + 0.5, i + 0.7, f'{percentage:.1f}%', \n",
    "                    ha='center', va='center', fontsize=12, \n",
    "                    color='red', fontweight='bold')\n",
    "    \n",
    "    # Add medical interpretation text box\n",
    "    interpretation = f\"\"\"\n",
    "    Medical Interpretation:\n",
    "    â€¢ Sensitivity: {best_model_results[\"recall\"]:.1%} (Disease detection rate)\n",
    "    â€¢ Specificity: {best_model_results[\"recall\"]:.1%} (Healthy identification rate)\n",
    "    â€¢ False Negatives: {cm[1, 0]} (Missed disease cases)\n",
    "    â€¢ False Positives: {cm[0, 1]} (Unnecessary referrals)\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figtext(0.02, 0.02, interpretation, fontsize=10, \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save high-resolution image\n",
    "    plt.savefig('confusion_matrix_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"     Confusion matrix heatmap saved as 'confusion_matrix_heatmap.png'\")\n",
    "\n",
    "create_confusion_matrix_heatmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9deb5c",
   "metadata": {},
   "source": [
    "## Model Comparison chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5e0968",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_model_comparison_chart():\n",
    "    \"\"\"\n",
    "    Create stunning model comparison visualization\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    models = [result['model_name'] for result in all_results]\n",
    "    accuracies = [result['accuracy'] for result in all_results]\n",
    "    f1_scores = [result['f1_score'] for result in all_results]\n",
    "    precisions = [result['precision'] for result in all_results]\n",
    "    recalls = [result['recall'] for result in all_results]\n",
    "    \n",
    "    # Create grouped bar chart\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.2\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    # Create bars\n",
    "    bars1 = ax.bar(x - 1.5*width, accuracies, width, label='Accuracy', \n",
    "                   color=COLORS['primary'], alpha=0.8)\n",
    "    bars2 = ax.bar(x - 0.5*width, f1_scores, width, label='F1-Score', \n",
    "                   color=COLORS['secondary'], alpha=0.8)\n",
    "    bars3 = ax.bar(x + 0.5*width, precisions, width, label='Precision', \n",
    "                   color=COLORS['success'], alpha=0.8)\n",
    "    bars4 = ax.bar(x + 1.5*width, recalls, width, label='Recall', \n",
    "                   color=COLORS['danger'], alpha=0.8)\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_xlabel('Machine Learning Models', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Performance Score', fontsize=14, fontweight='bold')\n",
    "    ax.set_title('Model Performance Comparison - Heart Disease Prediction\\n'\n",
    "                 'Higher scores indicate better performance', \n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "    ax.legend(loc='lower right', frameon=True, fancybox=True, shadow=True)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    def add_value_labels(bars):\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    add_value_labels(bars1)\n",
    "    add_value_labels(bars2)\n",
    "    add_value_labels(bars3)\n",
    "    add_value_labels(bars4)\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    \n",
    "    # Highlight best performing model\n",
    "    best_model_idx = np.argmax(f1_scores)\n",
    "    ax.text(best_model_idx, max(f1_scores) + 0.05, 'ðŸ† BEST', \n",
    "            ha='center', fontsize=12, fontweight='bold', color='gold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save high-resolution image\n",
    "    plt.savefig('model_comparison_chart.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"     Model comparison chart saved as 'model_comparison_chart.png'\")\n",
    "\n",
    "create_model_comparison_chart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd740f7",
   "metadata": {},
   "source": [
    "## Curve Visualizatin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e7eec3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_roc_curve_visualization():\n",
    "    \"\"\"\n",
    "    Create stunning ROC curve visualization\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot ROC curves for all models\n",
    "    for result in all_results:\n",
    "        fpr, tpr, _ = roc_curve(y_test, result['probabilities'])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.plot(fpr, tpr, \n",
    "                label=f\"{result['model_name']} (AUC = {roc_auc:.3f})\",\n",
    "                linewidth=3, alpha=0.8)\n",
    "    \n",
    "    # Plot diagonal reference line\n",
    "    plt.plot([0, 1], [0, 1], 'k--', linewidth=2, alpha=0.5, \n",
    "             label='Random Classifier (AUC = 0.500)')\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Positive Rate (Sensitivity)', fontsize=14, fontweight='bold')\n",
    "    plt.title('ROC Curves Comparison - Heart Disease Prediction Models\\n'\n",
    "              'Closer to top-left corner = Better Performance', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Add legend with custom positioning\n",
    "    plt.legend(loc=\"lower right\", frameon=True, fancybox=True, shadow=True, \n",
    "               fontsize=12)\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add AUC interpretation text\n",
    "    auc_interpretation = \"\"\"\n",
    "    AUC Interpretation:\n",
    "    â€¢ 0.90-1.00 = Excellent\n",
    "    â€¢ 0.80-0.90 = Good\n",
    "    â€¢ 0.70-0.80 = Fair\n",
    "    â€¢ 0.50-0.70 = Poor\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figtext(0.02, 0.02, auc_interpretation, fontsize=10, \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightyellow\"))\n",
    "    \n",
    "    # Highlight best AUC\n",
    "    best_auc_idx = np.argmax([auc(*roc_curve(y_test, result['probabilities'][:2])) \n",
    "                              for result in all_results])\n",
    "    best_result = all_results[best_auc_idx]\n",
    "    fpr_best, tpr_best, _ = roc_curve(y_test, best_result['probabilities'])\n",
    "    \n",
    "    # Add star to best model\n",
    "    plt.scatter(fpr_best[::10], tpr_best[::10], marker='*', s=100, \n",
    "                color='gold', edgecolors='black', linewidth=1,\n",
    "                label=f'â­ {best_result[\"model_name\"]} Points')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save high-resolution image\n",
    "    plt.savefig('roc_curves_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"     ROC curves saved as 'roc_curves_comparison.png'\")\n",
    "\n",
    "create_roc_curve_visualization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668bd3be",
   "metadata": {},
   "source": [
    "## Feature correlation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de93712",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_correlation_heatmap():\n",
    "    \"\"\"\n",
    "    Create beautiful feature correlation heatmap\n",
    "    \"\"\"\n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = X_train.corr()\n",
    "    \n",
    "    # Create mask for upper triangle (avoid duplication)\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    \n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(correlation_matrix, \n",
    "                mask=mask,\n",
    "                annot=True, \n",
    "                fmt='.2f', \n",
    "                cmap='RdBu_r', \n",
    "                center=0,\n",
    "                square=True,\n",
    "                linewidths=0.5,\n",
    "                cbar_kws={\"shrink\": 0.8, \"label\": \"Correlation Coefficient\"},\n",
    "                annot_kws={'size': 8})\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title('Feature Correlation Matrix - Heart Disease Dataset\\n'\n",
    "              'Red = Positive Correlation, Blue = Negative Correlation', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Rotate labels for better readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # Add interpretation\n",
    "    interpretation = \"\"\"\n",
    "    Key Insights:\n",
    "    â€¢ Strong correlations (>|0.5|) indicate related health metrics\n",
    "    â€¢ Chest pain (cp) shows high correlation with target\n",
    "    â€¢ Age correlates with several risk factors\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figtext(0.02, 0.02, interpretation, fontsize=10, \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.7))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save high-resolution image\n",
    "    plt.savefig('correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"     Correlation heatmap saved as 'correlation_heatmap.png'\")\n",
    "\n",
    "create_correlation_heatmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc5fa0d",
   "metadata": {},
   "source": [
    "## Target Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de907bf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_target_distribution_chart():\n",
    "    \"\"\"\n",
    "    Create engaging target distribution visualization\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Pie chart\n",
    "    target_counts = df['target'].value_counts()\n",
    "    labels = ['No Heart Disease', 'Heart Disease']\n",
    "    colors = [COLORS['primary'], COLORS['danger']]\n",
    "    explode = (0.05, 0.05)  # Slightly separate slices\n",
    "    \n",
    "    wedges, texts, autotexts = ax1.pie(target_counts, labels=labels, autopct='%1.1f%%',\n",
    "                                       colors=colors, explode=explode, shadow=True,\n",
    "                                       startangle=90, textprops={'fontsize': 12})\n",
    "    \n",
    "    # Enhance pie chart text\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "        autotext.set_fontsize(14)\n",
    "    \n",
    "    ax1.set_title('Heart Disease Distribution\\n(Overall Population)', \n",
    "                  fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Bar chart with counts\n",
    "    bars = ax2.bar(labels, target_counts, color=colors, alpha=0.8, edgecolor='black')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, target_counts):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 2,\n",
    "                f'{count}\\n({count/len(df)*100:.1f}%)', \n",
    "                ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax2.set_ylabel('Number of Patients', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Patient Count by Heart Disease Status', \n",
    "                  fontsize=14, fontweight='bold', pad=20)\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add medical context\n",
    "    medical_note = f\"\"\"\n",
    "    Medical Context:\n",
    "    â€¢ Total Patients: {len(df)}\n",
    "    â€¢ With Heart Disease: {target_counts[1]} ({target_counts[1]/len(df)*100:.1f}%)\n",
    "    â€¢ Without Heart Disease: {target_counts[0]} ({target_counts[0]/len(df)*100:.1f}%)\n",
    "    \n",
    "    This represents a {'balanced' if abs(target_counts[0] - target_counts[1]) < len(df)*0.1 else 'slightly imbalanced'} dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figtext(0.02, 0.02, medical_note, fontsize=10, \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\", alpha=0.7))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save high-resolution image\n",
    "    plt.savefig('target_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"     Target distribution saved as 'target_distribution.png'\")\n",
    "\n",
    "create_target_distribution_chart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0ed4e9",
   "metadata": {},
   "source": [
    "## Age vs Heart Disease Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de15fb6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_age_analysis_chart():\n",
    "    \"\"\"\n",
    "    Create age-based heart disease analysis\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Age distribution by heart disease status\n",
    "    no_disease = df[df['target'] == 0]['age']\n",
    "    disease = df[df['target'] == 1]['age']\n",
    "    \n",
    "    ax1.hist(no_disease, bins=15, alpha=0.7, label='No Heart Disease', \n",
    "             color=COLORS['primary'], density=True)\n",
    "    ax1.hist(disease, bins=15, alpha=0.7, label='Heart Disease', \n",
    "             color=COLORS['danger'], density=True)\n",
    "    \n",
    "    ax1.set_xlabel('Age (years)', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Density', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('Age Distribution by Heart Disease Status', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add statistical information\n",
    "    ax1.axvline(no_disease.mean(), color=COLORS['primary'], linestyle='--', \n",
    "                label=f'Mean (No Disease): {no_disease.mean():.1f}')\n",
    "    ax1.axvline(disease.mean(), color=COLORS['danger'], linestyle='--', \n",
    "                label=f'Mean (Disease): {disease.mean():.1f}')\n",
    "    \n",
    "    # Age group analysis\n",
    "    age_groups = ['<40', '40-50', '50-60', '60-70', '70+']\n",
    "    disease_rates = []\n",
    "    \n",
    "    for i, age_group in enumerate(age_groups):\n",
    "        if i == 0:\n",
    "            subset = df[df['age'] < 40]\n",
    "        elif i == 1:\n",
    "            subset = df[(df['age'] >= 40) & (df['age'] < 50)]\n",
    "        elif i == 2:\n",
    "            subset = df[(df['age'] >= 50) & (df['age'] < 60)]\n",
    "        elif i == 3:\n",
    "            subset = df[(df['age'] >= 60) & (df['age'] < 70)]\n",
    "        else:\n",
    "            subset = df[df['age'] >= 70]\n",
    "        \n",
    "        if len(subset) > 0:\n",
    "            disease_rate = subset['target'].mean() * 100\n",
    "            disease_rates.append(disease_rate)\n",
    "        else:\n",
    "            disease_rates.append(0)\n",
    "    \n",
    "    # Create bar chart\n",
    "    bars = ax2.bar(age_groups, disease_rates, \n",
    "                   color=plt.cm.Reds(np.linspace(0.4, 1, len(age_groups))))\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, rate in zip(bars, disease_rates):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "                f'{rate:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax2.set_xlabel('Age Groups', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Heart Disease Rate (%)', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Heart Disease Rate by Age Group', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add trend line\n",
    "    x_pos = range(len(age_groups))\n",
    "    z = np.polyfit(x_pos, disease_rates, 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax2.plot(x_pos, p(x_pos), \"r--\", alpha=0.8, linewidth=2, \n",
    "             label=f'Trend (slope: {z[0]:.1f}%)')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Add insight\n",
    "    insight = f\"\"\"\n",
    "    Key Insights:\n",
    "    â€¢ Average age (No Disease): {no_disease.mean():.1f} years\n",
    "    â€¢ Average age (Disease): {disease.mean():.1f} years\n",
    "    â€¢ Disease rate increases with age\n",
    "    â€¢ Highest risk group: {age_groups[np.argmax(disease_rates)]}\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figtext(0.02, 0.02, insight, fontsize=10, \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightyellow\"))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save high-resolution image\n",
    "    plt.savefig('age_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Age analysis saved as 'age_analysis.png'\")\n",
    "\n",
    "create_age_analysis_chart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a86519",
   "metadata": {},
   "source": [
    "## Create a comprehensive results dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42efebd8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_results_dashboard():\n",
    "    \"\"\"\n",
    "    Create a comprehensive results dashboard\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    \n",
    "    # Create grid layout\n",
    "    gs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. Model Performance Comparison (top left)\n",
    "    ax1 = fig.add_subplot(gs[0, :2])\n",
    "    models = [result['model_name'] for result in all_results]\n",
    "    f1_scores = [result['f1_score'] for result in all_results]\n",
    "    \n",
    "    bars = ax1.bar(models, f1_scores, color=['skyblue', 'lightgreen', 'orange', 'pink'])\n",
    "    ax1.set_title('Model Performance (F1-Score)', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('F1-Score')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    # Highlight best model\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    bars[best_idx].set_color('gold')\n",
    "    bars[best_idx].set_edgecolor('black')\n",
    "    bars[best_idx].set_linewidth(2)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, score) in enumerate(zip(bars, f1_scores)):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "                f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Feature Importance (top right)\n",
    "    ax2 = fig.add_subplot(gs[0, 2:])\n",
    "    best_rf_model = rf_results['model']\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': best_rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=True).tail(8)\n",
    "    \n",
    "    ax2.barh(feature_importance['feature'], feature_importance['importance'])\n",
    "    ax2.set_title('Top Risk Factors', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Importance Score')\n",
    "    \n",
    "    # 3. ROC Curves (middle left)\n",
    "    ax3 = fig.add_subplot(gs[1, :2])\n",
    "    for result in all_results:\n",
    "        fpr, tpr, _ = roc_curve(y_test, result['probabilities'])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        ax3.plot(fpr, tpr, label=f\"{result['model_name']} (AUC={roc_auc:.3f})\")\n",
    "    \n",
    "    ax3.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    ax3.set_xlabel('False Positive Rate')\n",
    "    ax3.set_ylabel('True Positive Rate')\n",
    "    ax3.set_title('ROC Curves Comparison', fontsize=14, fontweight='bold')\n",
    "    ax3.legend(loc='lower right')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Target Distribution (middle right)\n",
    "    ax4 = fig.add_subplot(gs[1, 2:])\n",
    "    target_counts = df['target'].value_counts()\n",
    "    labels = ['No Disease', 'Disease']\n",
    "    colors = [COLORS['primary'], COLORS['danger']]\n",
    "    \n",
    "    wedges, texts, autotexts = ax4.pie(target_counts, labels=labels, autopct='%1.1f%%',\n",
    "                                       colors=colors, startangle=90)\n",
    "    ax4.set_title('Dataset Distribution', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 5. Age Analysis (bottom left)\n",
    "    ax5 = fig.add_subplot(gs[2, :2])\n",
    "    age_groups = ['<40', '40-50', '50-60', '60-70', '70+']\n",
    "    disease_rates = []\n",
    "    \n",
    "    for i, age_group in enumerate(age_groups):\n",
    "        if i == 0:\n",
    "            subset = df[df['age'] < 40]\n",
    "        elif i == 1:\n",
    "            subset = df[(df['age'] >= 40) & (df['age'] < 50)]\n",
    "        elif i == 2:\n",
    "            subset = df[(df['age'] >= 50) & (df['age'] < 60)]\n",
    "        elif i == 3:\n",
    "            subset = df[(df['age'] >= 60) & (df['age'] < 70)]\n",
    "        else:\n",
    "            subset = df[df['age'] >= 70]\n",
    "        \n",
    "        disease_rate = subset['target'].mean() * 100 if len(subset) > 0 else 0\n",
    "        disease_rates.append(disease_rate)\n",
    "    \n",
    "    ax5.bar(age_groups, disease_rates, color='coral')\n",
    "    ax5.set_title('Heart Disease Rate by Age Group', fontsize=14, fontweight='bold')\n",
    "    ax5.set_ylabel('Disease Rate (%)')\n",
    "    ax5.set_xlabel('Age Groups')\n",
    "    \n",
    "    # Add trend line\n",
    "    x_pos = range(len(age_groups))\n",
    "    z = np.polyfit(x_pos, disease_rates, 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax5.plot(x_pos, p(x_pos), \"r--\", alpha=0.8, linewidth=2)\n",
    "    \n",
    "    # 6. Confusion Matrix (bottom right)\n",
    "    ax6 = fig.add_subplot(gs[2, 2:])\n",
    "    best_model_results = max(all_results, key=lambda x: x['f1_score'])\n",
    "    cm = confusion_matrix(y_test, best_model_results['predictions'])\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax6,\n",
    "                xticklabels=['No Disease', 'Disease'],\n",
    "                yticklabels=['No Disease', ' fontsize=14, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    ax1.axvline(no_disease.mean(), color=COLORS['primary'], linestyle='--', \n",
    "                alpha=0.8, label=f'No Disease Mean: {no_disease.mean():.1f}')\n",
    "    ax1.axvline(disease.mean(), color=COLORS['danger'], linestyle='--', \n",
    "                alpha=0.8, label=f'Disease Mean: {disease.mean():.1f}')\n",
    "    \n",
    "    # Age group analysis\n",
    "    age_groups = pd.cut(df['age'], bins=[0, 40, 50, 60, 70, 100], \n",
    "                        labels=['<40', '40-50', '50-60', '60-70', '70+'])\n",
    "    age_disease_rate = df.groupby(age_groups)['target'].agg(['count', 'sum', 'mean'])\n",
    "    \n",
    "    x_pos = range(len(age_disease_rate))\n",
    "    bars = ax2.bar(x_pos, age_disease_rate['mean'] * 100, \n",
    "                   color=COLORS['secondary'], alpha=0.8, edgecolor='black')\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for i, (bar, count, total) in enumerate(zip(bars, age_disease_rate['sum'], age_disease_rate['count'])):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1,\n",
    "                f'{total} patients\\n{count} cases\\n({age_disease_rate[\"mean\"].iloc[i]*100:.1f}%)', \n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    ax2.set_xlabel('Age Groups', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Heart Disease Rate (%)', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Heart Disease Rate by Age Group', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax2.set_xticks(x_pos)\n",
    "    ax2.set_xticklabels(age_disease_rate.index)\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(range(len(age_disease_rate)), age_disease_rate['mean'] * 100, 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax2.plot(x_pos, p(x_pos), \"r--\", alpha=0.8, linewidth=2, \n",
    "             label=f'Trend: +{z[0]:.1f}% per age group')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save high-resolution image\n",
    "    plt.savefig('age_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Age analysis saved as 'age_analysis.png'\")\n",
    "\n",
    "create_age_analysis_chart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b690d2e1",
   "metadata": {},
   "source": [
    "## Create presentation-Ready Summary Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae39253",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_summary_dashboard():\n",
    "    \"\"\"\n",
    "    Create a comprehensive summary dashboard for presentations\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # Create a grid layout\n",
    "    gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. Model Performance Comparison (top left)\n",
    "    ax1 = fig.add_subplot(gs[0, 0:2])\n",
    "    models = [result['model_name'] for result in all_results]\n",
    "    accuracies = [result['accuracy'] for result in all_results]\n",
    "    f1_scores = [result['f1_score'] for result in all_results]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax1.bar(x - width/2, accuracies, width, label='Accuracy', \n",
    "                    color=COLORS['primary'], alpha=0.8)\n",
    "    bars2 = ax1.bar(x + width/2, f1_scores, width, label='F1-Score', \n",
    "                    color=COLORS['secondary'], alpha=0.8)\n",
    "    \n",
    "    ax1.set_xlabel('Models', fontweight='bold')\n",
    "    ax1.set_ylabel('Score', fontweight='bold')\n",
    "    ax1.set_title('Model Performance Comparison', fontweight='bold', fontsize=14)\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(models, rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # 2. Best Model Metrics (top right)\n",
    "    ax2 = fig.add_subplot(gs[0, 2:4])\n",
    "    best_model = max(all_results, key=lambda x: x['f1_score'])\n",
    "    \n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "    values = [best_model['accuracy'], best_model['precision'], \n",
    "              best_model['recall'], best_model['f1_score']]\n",
    "    \n",
    "    bars = ax2.barh(metrics, values, color=COLORS['success'], alpha=0.8)\n",
    "    ax2.set_xlabel('Score', fontweight='bold')\n",
    "    ax2.set_title(f'Best Model Performance: {best_model[\"model_name\"]}', \n",
    "                  fontweight='bold', fontsize=14)\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, values):\n",
    "        ax2.text(value + 0.02, bar.get_y() + bar.get_height()/2,\n",
    "                f'{value:.3f}', va='center', fontweight='bold')\n",
    "    \n",
    "    # 3. Feature Importance (middle left)\n",
    "    ax3 = fig.add_subplot(gs[1, 0:2])\n",
    "    \n",
    "    # Get feature importance from best model\n",
    "    if hasattr(best_model['model'], 'feature_importances_'):\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': X_train.columns,\n",
    "            'importance': best_model['model'].feature_importances_\n",
    "        }).sort_values('importance', ascending=True).tail(10)\n",
    "        \n",
    "        bars = ax3.barh(importance_df['feature'], importance_df['importance'],\n",
    "                        color=COLORS['accent'], alpha=0.8)\n",
    "        ax3.set_xlabel('Importance', fontweight='bold')\n",
    "        ax3.set_title('Top 10 Most Important Features', fontweight='bold', fontsize=14)\n",
    "        ax3.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # 4. ROC Curve (middle right)\n",
    "    ax4 = fig.add_subplot(gs[1, 2:4])\n",
    "    \n",
    "    for result in all_results:\n",
    "        fpr, tpr, _ = roc_curve(y_test, result['probabilities'])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        ax4.plot(fpr, tpr, label=f\"{result['model_name']} (AUC = {roc_auc:.3f})\",\n",
    "                linewidth=2)\n",
    "    \n",
    "    ax4.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random')\n",
    "    ax4.set_xlabel('False Positive Rate', fontweight='bold')\n",
    "    ax4.set_ylabel('True Positive Rate', fontweight='bold')\n",
    "    ax4.set_title('ROC Curves Comparison', fontweight='bold', fontsize=14)\n",
    "    ax4.legend(loc=\"lower right\", fontsize=9)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Dataset Statistics (bottom left)\n",
    "    ax5 = fig.add_subplot(gs[2, 0:2])\n",
    "    ax5.axis('off')\n",
    "    \n",
    "    stats_text = f\"\"\"\n",
    "    DATASET STATISTICS\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    Total Patients: {len(df):,}\n",
    "    Features Used: {X_train.shape[1]}\n",
    "    Training Samples: {len(X_train):,}\n",
    "    Test Samples: {len(X_test):,}\n",
    "    \n",
    "    HEART DISEASE DISTRIBUTION\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    With Disease: {df['target'].sum():,} ({df['target'].mean()*100:.1f}%)\n",
    "    Without Disease: {(df['target']==0).sum():,} ({(1-df['target'].mean())*100:.1f}%)\n",
    "    \n",
    "    BEST MODEL DETAILS\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    Algorithm: {best_model['model_name']}\n",
    "    Training Time: {best_model['training_time']:.3f}s\n",
    "    Cross-Validation Score: {best_model['cv_mean']:.3f} Â± {best_model['cv_std']:.3f}\n",
    "    \"\"\"\n",
    "    \n",
    "    ax5.text(0.05, 0.95, stats_text, transform=ax5.transAxes, \n",
    "             fontsize=11, verticalalignment='top',\n",
    "             bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightgray\", alpha=0.8))\n",
    "    \n",
    "    # 6. Clinical Metrics (bottom right)\n",
    "    ax6 = fig.add_subplot(gs[2, 2:4])\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    # Calculate clinical metrics\n",
    "    cm = confusion_matrix(y_test, best_model['predictions'])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    \n",
    "    clinical_text = f\"\"\"\n",
    "    CLINICAL PERFORMANCE METRICS\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    Sensitivity (Recall): {sensitivity:.3f}\n",
    "    â†’ Detects {sensitivity*100:.1f}% of disease cases\n",
    "    \n",
    "    Specificity: {specificity:.3f}\n",
    "    â†’ Correctly identifies {specificity*100:.1f}% of healthy patients\n",
    "    \n",
    "    Positive Predictive Value: {ppv:.3f}\n",
    "    â†’ {ppv*100:.1f}% of positive predictions are correct\n",
    "    \n",
    "    MISCLASSIFICATION ANALYSIS\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    False Positives: {fp} (unnecessary worry)\n",
    "    False Negatives: {fn} (missed cases)\n",
    "    Total Errors: {fp + fn} out of {len(y_test)}\n",
    "    \"\"\"\n",
    "    \n",
    "    ax6.text(0.05, 0.95, clinical_text, transform=ax6.transAxes, \n",
    "             fontsize=11, verticalalignment='top',\n",
    "             bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightyellow\", alpha=0.8))\n",
    "    \n",
    "    # Add main title\n",
    "    fig.suptitle('Heart Disease Prediction Model - Performance Dashboard', \n",
    "                 fontsize=20, fontweight='bold', y=0.98)\n",
    "    \n",
    "    # Add subtitle\n",
    "    fig.text(0.5, 0.94, 'Comprehensive Analysis and Evaluation Results', \n",
    "             ha='center', fontsize=14, style='italic')\n",
    "    \n",
    "    plt.savefig('performance_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Performance dashboard saved as 'performance_dashboard.png'\")\n",
    "\n",
    "create_summary_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c5805a",
   "metadata": {},
   "source": [
    "## Save all visualizations for Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cc4cc9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_visualization_summary():\n",
    "    \"\"\"\n",
    "    Create a summary of all visualizations created\n",
    "    \"\"\"\n",
    "    visualizations = {\n",
    "        'feature_importance_chart.png': 'Top 15 most important health metrics',\n",
    "        'confusion_matrix_heatmap.png': 'Model prediction accuracy breakdown',\n",
    "        'model_comparison_chart.png': 'Performance comparison across all models',\n",
    "        'roc_curves_comparison.png': 'ROC curves showing diagnostic ability',\n",
    "        'correlation_heatmap.png': 'Relationships between health factors',\n",
    "        'target_distribution.png': 'Heart disease prevalence in dataset',\n",
    "        'age_analysis.png': 'Age-related heart disease patterns',\n",
    "        'performance_dashboard.png': 'Complete performance summary'\n",
    "    }\n",
    "    \n",
    "    print(\"=== VISUALIZATION SUMMARY ===\")\n",
    "    print(\"All visualizations have been created and saved!\")\n",
    "    print(\"\\nFiles created:\")\n",
    "    for filename, description in visualizations.items():\n",
    "        print(f\"  ðŸ“Š {filename}: {description}\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ Usage Tips:\")\n",
    "    print(\"â€¢ Use 'feature_importance_chart.png' in your README.md\")\n",
    "    print(\"â€¢ Include 'performance_dashboard.png' in your presentation\")\n",
    "    print(\"â€¢ Add 'confusion_matrix_heatmap.png' to your article\")\n",
    "    print(\"â€¢ Reference 'model_comparison_chart.png' in your pitch deck\")\n",
    "    \n",
    "    # Create a markdown snippet for GitHub README\n",
    "    readme_snippet = \"\"\"\n",
    "    ## ðŸ“Š Model Performance Visualizations\n",
    "    \n",
    "    ### Feature Importance\n",
    "    ![Feature Importance](feature_importance_chart.png)\n",
    "    \n",
    "    ### Model Comparison\n",
    "    ![Model Comparison](model_comparison_chart.png)\n",
    "    \n",
    "    ### Confusion Matrix\n",
    "    ![Confusion Matrix](confusion_matrix_heatmap.png)\n",
    "    \n",
    "    ### ROC Curves\n",
    "    ![ROC Curves](roc_curves_comparison.png)\n",
    "    \n",
    "    ### Performance Dashboard\n",
    "    ![Performance Dashboard](performance_dashboard.png)\n",
    "    \"\"\"\n",
    "    \n",
    "    with open('visualization_readme_snippet.md', 'w') as f:\n",
    "        f.write(readme_snippet)\n",
    "    \n",
    "    print(\"\\nðŸ“„ 'visualization_readme_snippet.md' created for your GitHub README!\")\n",
    "\n",
    "create_visualization_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5c69f4",
   "metadata": {},
   "source": [
    "## Color palette reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7722bbe6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create a color palette reference for consistent branding\n",
    "def create_color_palette_reference():\n",
    "    \"\"\"\n",
    "    Show the color palette used for consistent visualizations\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    colors = list(COLORS.values())\n",
    "    color_names = list(COLORS.keys())\n",
    "    \n",
    "    # Create color swatches\n",
    "    for i, (color, name) in enumerate(zip(colors, color_names)):\n",
    "        ax.add_patch(plt.Rectangle((i, 0), 1, 1, facecolor=color, edgecolor='black'))\n",
    "        ax.text(i + 0.5, 0.5, name, ha='center', va='center', \n",
    "                color='white' if color in ['#e74c3c', '#34495e'] else 'black',\n",
    "                fontweight='bold', fontsize=12)\n",
    "    \n",
    "    ax.set_xlim(0, len(colors))\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title('Color Palette Used in Visualizations\\n(Perfect for presentations and reports)', \n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.savefig('color_palette_reference.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"ðŸ’¾ Color palette saved as 'color_palette_reference.png'\")\n",
    "\n",
    "create_color_palette_reference()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
