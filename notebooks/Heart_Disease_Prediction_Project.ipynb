{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages \n",
    "!pip install kaggle\n",
    "!pip install pandas numpy scikit-learn matplotlib seaborn\n",
    "\n",
    "# Import all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d35ade",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Direct download from UCI repository\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset directly from the corrected URL\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
    "# The dataset does not have a header row, and the columns are not named.\n",
    "# We need to provide column names manually based on the dataset description.\n",
    "column_names = [\n",
    "    'age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n",
    "    'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'\n",
    "]\n",
    "df = pd.read_csv(url, names=column_names, na_values=\"?\") # Handle missing values represented by '?'\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1778ef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('heart.csv') or you can use the variable from alternative method\n",
    "\n",
    "# 1. View first few rows\n",
    "print(\"=== First 5 rows of the dataset ===\")\n",
    "print(df.head())\n",
    "print(\"\\n\")\n",
    "\n",
    "# 2. Check dataset info (data types, non-null counts)\n",
    "print(\"=== Dataset Information ===\")\n",
    "print(df.info())\n",
    "print(\"\\n\")\n",
    "\n",
    "# 3. Summary statistics\n",
    "print(\"=== Summary Statistics ===\")\n",
    "print(df.describe())\n",
    "print(\"\\n\")\n",
    "\n",
    "# 4. Check for missing values\n",
    "print(\"=== Missing Values Check ===\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\n\")\n",
    "\n",
    "# 5. Check target variable distribution\n",
    "print(\"=== Target Variable Distribution ===\")\n",
    "print(df['target'].value_counts())\n",
    "print(f\"Percentage with heart disease: {(df['target'].sum()/len(df)*100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def6a0e1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check for missing values again after loading with '?' as NA\n",
    "print(\"=== Missing values after loading with na_values='?' ===\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Handle missing values: Fill missing values in 'ca' and 'thal' with the mode\n",
    "for col in ['ca', 'thal']:\n",
    "    if df[col].isnull().any():\n",
    "        mode_value = df[col].mode()[0]\n",
    "        df[col].fillna(mode_value, inplace=True)\n",
    "        print(f\"Filled missing values in '{col}' with the mode: {mode_value}\")\n",
    "\n",
    "print(\"\\n=== Missing values after handling ===\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edec764d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Converts columns to appropriate data types\n",
    "# 'ca' and 'thal' are currently float due to missing values, convert them to int\n",
    "for col in ['ca', 'thal', 'target']:\n",
    "    df[col] = df[col].astype(int)\n",
    "\n",
    "print(\"=== Data types after conversion ===\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fad11e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Explore the distribution of categorical features\n",
    "categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal', 'target']\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(categorical_features):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    df[col].value_counts().plot(kind='bar', color=sns.color_palette('viridis'))\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd1665",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(numerical_features):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    sns.histplot(df[col], kde=True, color=sns.color_palette('viridis')[i])\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72509489",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create a feature dictionary for reference\n",
    "feature_dict = {\n",
    "    'age': 'Age in years',\n",
    "    'sex': 'Sex (1 = male, 0 = female)',\n",
    "    'cp': 'Chest pain type (0-3)',\n",
    "    'trestbps': 'Resting blood pressure (mm Hg)',\n",
    "    'chol': 'Serum cholesterol (mg/dl)',\n",
    "    'fbs': 'Fasting blood sugar > 120 mg/dl (1 = true, 0 = false)',\n",
    "    'restecg': 'Resting ECG results (0-2)',\n",
    "    'thalach': 'Maximum heart rate achieved',\n",
    "    'exang': 'Exercise induced angina (1 = yes, 0 = no)',\n",
    "    'oldpeak': 'ST depression induced by exercise',\n",
    "    'slope': 'Slope of peak exercise ST segment (0-2)',\n",
    "    'ca': 'Number of major vessels colored by fluoroscopy (0-3)',\n",
    "    'thal': 'Thalassemia (0 = normal, 1 = fixed defect, 2 = reversable defect)',\n",
    "    'target': 'Heart disease presence (1 = yes, 0 = no)'\n",
    "}\n",
    "\n",
    "print(\"=== Feature Descriptions ===\")\n",
    "for feature, description in feature_dict.items():\n",
    "    print(f\"{feature}: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6166c2dc",
   "metadata": {},
   "source": [
    "A solution: install kaggle package first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a5f764",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade kaggle\n",
    "!kaggle datasets download -v -d ronitf/heart-disease-uci --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326aac7a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "for fn in uploaded.keys():\n",
    "    print(f'User uploaded file \"{fn}\" with length {len(uploaded[fn])} bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38eed90",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create a simple visualization to understand the data\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Subplot 1: Target distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "df['target'].value_counts().plot(kind='bar', color=['lightcoral', 'lightblue'])\n",
    "plt.title('Heart Disease Distribution')\n",
    "plt.xlabel('Target (0=No Disease, 1=Disease)')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Subplot 2: Age distribution by target\n",
    "plt.subplot(1, 2, 2)\n",
    "df[df['target']==0]['age'].hist(alpha=0.7, label='No Disease', color='lightblue')\n",
    "df[df['target']==1]['age'].hist(alpha=0.7, label='Disease', color='lightcoral')\n",
    "plt.title('Age Distribution by Heart Disease Status')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e22d178",
   "metadata": {},
   "source": [
    "Task 2 Starts: Data preprocessing => cleaning and preparing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb7d5b9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# data quality assessment\n",
    "print(\"=== DATA QUALITY ASSESSMENT ===\\n\")\n",
    "\n",
    "# 1. Check dataset shape\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of patients: {df.shape[0]}\")\n",
    "print(f\"Number of features: {df.shape[1] - 1} (excluding target)\")\n",
    "\n",
    "# 2. Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Duplicate rows: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    print(\"Removing duplicates...\")\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"New shape after removing duplicates: {df.shape}\")\n",
    "\n",
    "# 3. Check for missing values (detailed)\n",
    "print(\"\\n=== Missing Values Analysis ===\")\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "missing_table = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Percentage': missing_percent\n",
    "})\n",
    "print(missing_table[missing_table['Missing Count'] > 0])\n",
    "\n",
    "# 4. Check data types\n",
    "print(\"\\n=== Data Types Check ===\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a52e29",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=== FEATURE ANALYSIS ===\\n\")\n",
    "\n",
    "# Create a detailed feature analysis\n",
    "for column in df.columns:\n",
    "    if column != 'target':\n",
    "        print(f\"\\n--- {column.upper()} ---\")\n",
    "        print(f\"Data type: {df[column].dtype}\")\n",
    "        print(f\"Unique values: {df[column].nunique()}\")\n",
    "        print(f\"Min: {df[column].min()}, Max: {df[column].max()}\")\n",
    "        print(f\"Value counts:\\n{df[column].value_counts().head(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cae85f",
   "metadata": {},
   "source": [
    "# Even though UCI dataset typically has no missing values, \n",
    "# here's how to handle them if they exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b8e56d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def handle_missing_values(df):\n",
    "    \"\"\"Comprehensive missing value handling\"\"\"\n",
    "    print(\"=== HANDLING MISSING VALUES ===\")\n",
    "    \n",
    "    # Check for missing values again\n",
    "    missing = df.isnull().sum()\n",
    "    \n",
    "    if missing.sum() == 0:\n",
    "        print(\"No missing values found!\")\n",
    "        return df\n",
    "    \n",
    "    # Strategy depends on the feature type\n",
    "    for column in df.columns:\n",
    "        if df[column].isnull().sum() > 0:\n",
    "            print(f\"\\nHandling missing values in {column}:\")\n",
    "            \n",
    "            # For numerical features\n",
    "            if df[column].dtype in ['int64', 'float64']:\n",
    "                # Use median (more robust than mean)\n",
    "                median_value = df[column].median()\n",
    "                df[column].fillna(median_value, inplace=True)\n",
    "                print(f\"  - Filled {df[column].isnull().sum()} missing values with median: {median_value}\")\n",
    "            \n",
    "            # For categorical features\n",
    "            else:\n",
    "                # Use mode (most frequent value)\n",
    "                mode_value = df[column].mode()[0]\n",
    "                df[column].fillna(mode_value, inplace=True)\n",
    "                print(f\"  - Filled {df[column].isnull().sum()} missing values with mode: {mode_value}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function\n",
    "df = handle_missing_values(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ba2319",
   "metadata": {},
   "source": [
    "# Outlier detection and handling \n",
    "# Detect outliers using statistical methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82319dab",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def detect_outliers(df, features):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    print(\"=== OUTLIER DETECTION ===\")\n",
    "    \n",
    "    outlier_indices = []\n",
    "    \n",
    "    for feature in features:\n",
    "        if feature != 'target' and df[feature].dtype in ['int64', 'float64']:\n",
    "            # Calculate IQR\n",
    "            Q1 = df[feature].quantile(0.25)\n",
    "            Q3 = df[feature].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            # Define outlier bounds\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            # Find outliers\n",
    "            outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n",
    "            \n",
    "            if len(outliers) > 0:\n",
    "                print(f\"\\n{feature}:\")\n",
    "                print(f\"  - Lower bound: {lower_bound:.2f}\")\n",
    "                print(f\"  - Upper bound: {upper_bound:.2f}\")\n",
    "                print(f\"  - Outliers found: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)\")\n",
    "                \n",
    "                # Visualize outliers\n",
    "                plt.figure(figsize=(10, 4))\n",
    "                \n",
    "                plt.subplot(1, 2, 1)\n",
    "                df[feature].hist(bins=30, alpha=0.7)\n",
    "                plt.axvline(lower_bound, color='red', linestyle='--', label=f'Lower bound: {lower_bound:.1f}')\n",
    "                plt.axvline(upper_bound, color='red', linestyle='--', label=f'Upper bound: {upper_bound:.1f}')\n",
    "                plt.title(f'{feature} Distribution with Outlier Bounds')\n",
    "                plt.legend()\n",
    "                \n",
    "                plt.subplot(1, 2, 2)\n",
    "                df.boxplot(column=feature)\n",
    "                plt.title(f'{feature} Boxplot')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "    \n",
    "    return outlier_indices\n",
    "\n",
    "# Apply outlier detection\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "detect_outliers(df, numerical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61fc696",
   "metadata": {},
   "source": [
    "# Feature engineering and selection\n",
    "# Create new features that might be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ad4a9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"=== FEATURE ENGINEERING ===\")\n",
    "\n",
    "# 1. Age groups\n",
    "df['age_group'] = pd.cut(df['age'], \n",
    "                        bins=[0, 40, 50, 60, 70, 100], \n",
    "                        labels=['<40', '40-50', '50-60', '60-70', '70+'])\n",
    "\n",
    "# 2. Cholesterol categories (using medical standards)\n",
    "df['chol_category'] = pd.cut(df['chol'], \n",
    "                            bins=[0, 200, 240, 1000], \n",
    "                            labels=['Desirable', 'Borderline', 'High'])\n",
    "\n",
    "# 3. Blood pressure categories\n",
    "df['bp_category'] = pd.cut(df['trestbps'], \n",
    "                          bins=[0, 120, 130, 140, 180, 300], \n",
    "                          labels=['Normal', 'Elevated', 'Stage1', 'Stage2', 'Crisis'])\n",
    "\n",
    "# 4. Heart rate efficiency (thalach vs age)\n",
    "df['heart_rate_efficiency'] = df['thalach'] / df['age']\n",
    "\n",
    "print(\"New features created:\")\n",
    "print(\"- age_group: Categorized age ranges\")\n",
    "print(\"- chol_category: Cholesterol levels based on medical standards\")\n",
    "print(\"- bp_category: Blood pressure categories\")\n",
    "print(\"- heart_rate_efficiency: Ratio of max heart rate to age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59b67ab",
   "metadata": {},
   "source": [
    "# Data normalization and scalling\n",
    "# Separate features and target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc82e14",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "print(\"=== FEATURE SCALING ===\")\n",
    "\n",
    "# Identify numerical and categorical features\n",
    "numerical_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numerical features: {len(numerical_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")\n",
    "\n",
    "# Handle categorical features (if any)\n",
    "if categorical_features:\n",
    "    X = pd.get_dummies(X, columns=categorical_features, drop_first=True)\n",
    "    print(f\"After one-hot encoding: {X.shape[1]} features\")\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X.copy()\n",
    "X_scaled[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "\n",
    "print(\"\\nScaling completed!\")\n",
    "print(f\"Original feature range example (age): {X['age'].min():.1f} to {X['age'].max():.1f}\")\n",
    "print(f\"Scaled feature range example (age): {X_scaled['age'].min():.2f} to {X_scaled['age'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e253d4ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"=== TRAIN-TEST SPLIT ===\")\n",
    "\n",
    "# Split the data (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Training set distribution: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Test set distribution: {y_test.value_counts().to_dict()}\")\n",
    "\n",
    "# Verify the split maintains the class distribution\n",
    "train_ratio = y_train.sum() / len(y_train)\n",
    "test_ratio = y_test.sum() / len(y_test)\n",
    "print(f\"\\nClass balance check:\")\n",
    "print(f\"Training set positive ratio: {train_ratio:.3f}\")\n",
    "print(f\"Test set positive ratio: {test_ratio:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ac65d0",
   "metadata": {},
   "source": [
    "# Final Data verification \n",
    "# Create a comprehensive verification report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c327ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=== FINAL VERIFICATION ===\\n\")\n",
    "\n",
    "def verify_preprocessing(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Verify that preprocessing was successful\"\"\"\n",
    "    \n",
    "    # 1. Check shapes\n",
    "    print(\"1. Shape Verification:\")\n",
    "    print(f\"   X_train: {X_train.shape}\")\n",
    "    print(f\"   X_test: {X_test.shape}\")\n",
    "    print(f\"   y_train: {y_train.shape}\")\n",
    "    print(f\"   y_test: {y_test.shape}\")\n",
    "    \n",
    "    # 2. Check for missing values\n",
    "    print(\"\\n2. Missing Values Check:\")\n",
    "    print(f\"   X_train missing: {X_train.isnull().sum().sum()}\")\n",
    "    print(f\"   X_test missing: {X_test.isnull().sum().sum()}\")\n",
    "    \n",
    "    # 3. Check scaling\n",
    "    print(\"\\n3. Scaling Verification (first 3 numerical features):\")\n",
    "    numerical_cols = X_train.select_dtypes(include=[np.number]).columns[:3]\n",
    "    for col in numerical_cols:\n",
    "        print(f\"   {col}: mean={X_train[col].mean():.3f}, std={X_train[col].std():.3f}\")\n",
    "    \n",
    "    # 4. Check target distribution\n",
    "    print(\"\\n4. Target Distribution:\")\n",
    "    print(f\"   Training: {y_train.value_counts().to_dict()}\")\n",
    "    print(f\"   Testing: {y_test.value_counts().to_dict()}\")\n",
    "    \n",
    "    print(\"\\nâœ… Preprocessing completed successfully!\")\n",
    "\n",
    "# Run verification\n",
    "verify_preprocessing(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b33fb1d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the scaler for later use\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Save the preprocessed data\n",
    "preprocessed_data = {\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'feature_names': X_train.columns.tolist()\n",
    "}\n",
    "\n",
    "with open('preprocessed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessed_data, f)\n",
    "\n",
    "print(\"Preprocessed data saved!\")\n",
    "print(\"Files created: scaler.pkl, preprocessed_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4123f5",
   "metadata": {},
   "source": [
    "# Summary Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64ef700",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the scaler for later use\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Save the preprocessed data\n",
    "preprocessed_data = {\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'feature_names': X_train.columns.tolist()\n",
    "}\n",
    "\n",
    "with open('preprocessed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessed_data, f)\n",
    "\n",
    "print(\"Preprocessed data saved!\")\n",
    "print(\"Files created: scaler.pkl, preprocessed_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc70f2b",
   "metadata": {},
   "source": [
    "# Check if one-hot encoding created too many features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a8746c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Original features: {X.shape[1]}\")\n",
    "print(f\"After preprocessing: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227b7a06",
   "metadata": {},
   "source": [
    "# Task 3: Model Building - Training Multiple Classification Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b7f48",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import additional libraries needed for modeling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "import time\n",
    "\n",
    "print(\"=== MODEL BUILDING SETUP ===\")\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "print(f\"Features: {list(X_train.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e41d7f",
   "metadata": {},
   "source": [
    "# create a model training framework \n",
    "# Create a function to train and evaluate models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcf1965",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Train a model and return performance metrics\n",
    "    \n",
    "    Parameters:\n",
    "    model: sklearn model instance\n",
    "    X_train: training features\n",
    "    y_train: training target\n",
    "    X_test: test features\n",
    "    y_test: test target\n",
    "    model_name: name of the model for display\n",
    "    \n",
    "    Returns:\n",
    "    Dictionary with model results\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Training {model_name} ===\")\n",
    "    \n",
    "    # Record training time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'model': model,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'training_time': training_time\n",
    "    }\n",
    "    \n",
    "    print(f\"Training completed in {training_time:.3f} seconds\")\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"Cross-validation: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a958ea",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Model 1 - Logistic Regression\n",
    "print(\"=== MODEL 1: LOGISTIC REGRESSION ===\")\n",
    "print(\"Why Logistic Regression?\")\n",
    "print(\"- Simple and interpretable\")\n",
    "print(\"- Good baseline for binary classification\")\n",
    "print(\"- Provides probability estimates\")\n",
    "print(\"- Fast training\")\n",
    "\n",
    "# Initialize Logistic Regression\n",
    "lr_model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,  # Increase iterations for convergence\n",
    "    solver='liblinear'  # Good for small datasets\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "lr_results = train_model(lr_model, X_train, y_train, X_test, y_test, \"Logistic Regression\")\n",
    "\n",
    "# Show feature importance (coefficients)\n",
    "feature_importance_lr = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'coefficient': lr_model.coef_[0],\n",
    "    'abs_coefficient': np.abs(lr_model.coef_[0])\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features (Logistic Regression):\")\n",
    "print(feature_importance_lr.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af86a3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Model 2 - Random Forest\n",
    "print(\"\\n=== MODEL 2: RANDOM FOREST ===\")\n",
    "print(\"Why Random Forest?\")\n",
    "print(\"- Handles non-linear relationships\")\n",
    "print(\"- Robust to outliers\")\n",
    "print(\"- Provides feature importance\")\n",
    "print(\"- Good for mixed data types\")\n",
    "\n",
    "# Initialize Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,  # Number of trees\n",
    "    random_state=42,\n",
    "    max_depth=10,  # Prevent overfitting\n",
    "    min_samples_split=5,  # Minimum samples to split a node\n",
    "    min_samples_leaf=2,   # Minimum samples in leaf node\n",
    "    class_weight='balanced'  # Handle class imbalance\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_results = train_model(rf_model, X_train, y_train, X_test, y_test, \"Random Forest\")\n",
    "\n",
    "# Show feature importance\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features (Random Forest):\")\n",
    "print(feature_importance_rf.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f441efb9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Model 3 - K-Nearest Neighbors \n",
    "print(\"\\n=== MODEL 3: K-NEAREST NEIGHBORS ===\")\n",
    "print(\"Why KNN?\")\n",
    "print(\"- Simple and intuitive\")\n",
    "print(\"- Non-parametric (makes no assumptions)\")\n",
    "print(\"- Good for small datasets\")\n",
    "print(\"- Instance-based learning\")\n",
    "\n",
    "# We need to find optimal k value first\n",
    "print(\"\\nFinding optimal k value...\")\n",
    "\n",
    "# Test different k values\n",
    "k_values = range(1, 21)\n",
    "cv_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "# Find optimal k\n",
    "optimal_k = k_values[np.argmax(cv_scores)]\n",
    "print(f\"Optimal k value: {optimal_k}\")\n",
    "\n",
    "# Plot k vs accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, cv_scores, marker='o')\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Cross-Validation Accuracy')\n",
    "plt.title('K Value Optimization for KNN')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Train with optimal k\n",
    "knn_model = KNeighborsClassifier(\n",
    "    n_neighbors=optimal_k,\n",
    "    weights='distance',  # Weight by distance (closer neighbors matter more)\n",
    "    metric='euclidean'   # Distance metric\n",
    ")\n",
    "\n",
    "knn_results = train_model(knn_model, X_train, y_train, X_test, y_test, f\"KNN (k={optimal_k})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7961aac4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Model 4 - Neural Network \n",
    "print(\"\\n=== MODEL 4: NEURAL NETWORK ===\")\n",
    "print(\"Why Neural Network?\")\n",
    "print(\"- Can learn complex patterns\")\n",
    "print(\"- Good for non-linear relationships\")\n",
    "print(\"- Scalable to large datasets\")\n",
    "\n",
    "# Import MLPClassifier (Multi-layer Perceptron)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Initialize Neural Network\n",
    "nn_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),  # Two hidden layers\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    early_stopping=True,  # Prevent overfitting\n",
    "    validation_fraction=0.1,\n",
    "    alpha=0.01,  # L2 regularization\n",
    "    learning_rate_init=0.001\n",
    ")\n",
    "\n",
    "print(\"Training Neural Network (this may take a moment)...\")\n",
    "nn_results = train_model(nn_model, X_train, y_train, X_test, y_test, \"Neural Network\")\n",
    "\n",
    "# Show training loss curve\n",
    "if hasattr(nn_model, 'loss_curve_'):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(nn_model.loss_curve_)\n",
    "    plt.title('Neural Network Training Loss')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7664582",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "all_results = [lr_results, rf_results, knn_results, nn_results]\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': [result['model_name'] for result in all_results],\n",
    "    'Accuracy': [result['accuracy'] for result in all_results],\n",
    "    'Precision': [result['precision'] for result in all_results],\n",
    "    'Recall': [result['recall'] for result in all_results],\n",
    "    'F1-Score': [result['f1_score'] for result in all_results],\n",
    "    'CV Mean': [result['cv_mean'] for result in all_results],\n",
    "    'CV Std': [result['cv_std'] for result in all_results],\n",
    "    'Training Time (s)': [result['training_time'] for result in all_results]\n",
    "})\n",
    "\n",
    "print(\"=== MODEL COMPARISON ===\")\n",
    "print(comparison_df.round(3))\n",
    "\n",
    "# Find best model\n",
    "best_model_idx = np.argmax([result['f1_score'] for result in all_results])\n",
    "best_model = all_results[best_model_idx]\n",
    "print(f\"\\nBest Model: {best_model['model_name']}\")\n",
    "print(f\"F1-Score: {best_model['f1_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1856c05",
   "metadata": {},
   "source": [
    "# Improving the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6330ea4b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# improve the best model with hyperparameter tuning\n",
    "print(f\"\\n=== HYPERPARAMETER TUNING FOR {best_model['model_name']} ===\")\n",
    "\n",
    "# Define parameter grids for each model type\n",
    "if 'Logistic Regression' in best_model['model_name']:\n",
    "    param_grid = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear']\n",
    "    }\n",
    "elif 'Random Forest' in best_model['model_name']:\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [5, 10, 15, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "elif 'KNN' in best_model['model_name']:\n",
    "    param_grid = {\n",
    "        'n_neighbors': range(3, 20, 2),\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "    }\n",
    "\n",
    "# Perform Grid Search (using a smaller parameter set for speed)\n",
    "print(\"Performing grid search...\")\n",
    "grid_search = GridSearchCV(\n",
    "    best_model['model'].__class__(),\n",
    "    param_grid,\n",
    "    cv=3,  # 3-fold CV for speed\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "# Train final model with best parameters\n",
    "final_model = grid_search.best_estimator_\n",
    "final_results = train_model(final_model, X_train, y_train, X_test, y_test, \n",
    "                           f\"Tuned {best_model['model_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64583eaa",
   "metadata": {},
   "source": [
    "# Save the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa17e2aa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Save the best performing model\n",
    "import joblib\n",
    "\n",
    "# Determine which model to save\n",
    "if final_results['f1_score'] > best_model['f1_score']:\n",
    "    model_to_save = final_model\n",
    "    model_name = f\"tuned_{best_model['model_name'].lower().replace(' ', '_')}\"\n",
    "else:\n",
    "    model_to_save = best_model['model']\n",
    "    model_name = best_model['model_name'].lower().replace(' ', '_')\n",
    "\n",
    "# Save model\n",
    "model_filename = f'best_heart_disease_model_{model_name}.pkl'\n",
    "joblib.dump(model_to_save, model_filename)\n",
    "\n",
    "# Save scaler (we'll need it for new predictions)\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "print(f\"\\nðŸ’¾ Model saved as: {model_filename}\")\n",
    "print(f\"ðŸ’¾ Scaler saved as: scaler.pkl\")\n",
    "\n",
    "# Create a model summary\n",
    "model_summary = {\n",
    "    'best_model': model_name,\n",
    "    'accuracy': max(final_results['accuracy'], best_model['accuracy']),\n",
    "    'precision': max(final_results['precision'], best_model['precision']),\n",
    "    'recall': max(final_results['recall'], best_model['recall']),\n",
    "    'f1_score': max(final_results['f1_score'], best_model['f1_score']),\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'features_used': list(X_train.columns)\n",
    "}\n",
    "\n",
    "print(\"\\n=== FINAL MODEL SUMMARY ===\")\n",
    "for key, value in model_summary.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ce998d",
   "metadata": {},
   "source": [
    "# Create comprehensive feature importance visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df537df7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Get feature importance from the best model\n",
    "if hasattr(model_to_save, 'feature_importances_'):  # Tree-based models\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': model_to_save.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    # Plot top 15 features\n",
    "    plt.subplot(2, 2, 1)\n",
    "    top_features = importance_df.head(15)\n",
    "    plt.barh(top_features['feature'], top_features['importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title(f'Top 15 Features - {model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "elif hasattr(model_to_save, 'coef_'):  # Linear models\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'coefficient': model_to_save.coef_[0],\n",
    "        'abs_coefficient': np.abs(model_to_save.coef_[0])\n",
    "    }).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "    # Plot top 15 features\n",
    "    plt.subplot(2, 2, 1)\n",
    "    top_features = importance_df.head(15)\n",
    "    colors = ['red' if x < 0 else 'blue' for x in top_features['coefficient']]\n",
    "    plt.barh(top_features['feature'], top_features['coefficient'], color=colors)\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.title(f'Top 15 Features - {model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "# Plot model comparison\n",
    "plt.subplot(2, 2, 2)\n",
    "models = [result['model_name'] for result in all_results]\n",
    "accuracies = [result['accuracy'] for result in all_results]\n",
    "plt.bar(models, accuracies, color=['skyblue', 'lightgreen', 'orange', 'pink'])\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Plot F1-score comparison\n",
    "plt.subplot(2, 2, 3)\n",
    "f1_scores = [result['f1_score'] for result in all_results]\n",
    "plt.bar(models, f1_scores, color=['skyblue', 'lightgreen', 'orange', 'pink'])\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.title('Model F1-Score Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Plot training time comparison\n",
    "plt.subplot(2, 2, 4)\n",
    "training_times = [result['training_time'] for result in all_results]\n",
    "plt.bar(models, training_times, color=['skyblue', 'lightgreen', 'orange', 'pink'])\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Training Time (seconds)')\n",
    "plt.title('Training Time Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68eff77",
   "metadata": {},
   "source": [
    "# visualization for logistic regression and neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ff4089",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# For Logistic Regression\n",
    "LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "\n",
    "# For Neural Network\n",
    "MLPClassifier(max_iter=2000, early_stopping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dee4427",
   "metadata": {},
   "source": [
    "# visualization for random forest - reduce complexity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0a0e6e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Random Forest - reduce complexity\n",
    "RandomForestClassifier(max_depth=10, min_samples_split=5, min_samples_leaf=2)\n",
    "\n",
    "# Add regularization\n",
    "LogisticRegression(C=0.1, penalty='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229e15db",
   "metadata": {},
   "source": [
    "# Use class_weight parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308cc27d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Use class_weight parameter\n",
    "RandomForestClassifier(class_weight='balanced')\n",
    "\n",
    "# Or use SMOTE for oversampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdc6394",
   "metadata": {},
   "source": [
    "# Model evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb226723",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import all necessary evaluation libraries\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                           f1_score, confusion_matrix, classification_report,\n",
    "                           roc_curve, auc, precision_recall_curve)\n",
    "from sklearn.metrics import matthews_corrcoef, cohen_kappa_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=== MODEL EVALUATION SETUP ===\")\n",
    "print(\"Models to evaluate:\")\n",
    "for i, result in enumerate(all_results):\n",
    "    print(f\"{i+1}. {result['model_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b345dd61",
   "metadata": {},
   "source": [
    "# metrics calculation for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0585c76",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_comprehensive_metrics(y_true, y_pred, y_pred_proba, model_name):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive evaluation metrics\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with all metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== {model_name} - Detailed Metrics ===\")\n",
    "\n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    # Advanced metrics\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)  # Matthews Correlation Coefficient\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)  # Cohen's Kappa\n",
    "\n",
    "    # Classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['No Disease', 'Disease']))\n",
    "\n",
    "    metrics_dict = {\n",
    "        'model_name': model_name,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'mcc': mcc,\n",
    "        'kappa': kappa,\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "# Calculate metrics for all models\n",
    "all_metrics = []\n",
    "for result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38561075",
   "metadata": {},
   "source": [
    "# confusion matrix interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52b2784",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(all_metrics):\n",
    "    \"\"\"\n",
    "    Create confusion matrices for all models\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i, metrics in enumerate(all_metrics):\n",
    "        cm = confusion_matrix(metrics['y_true'], metrics['y_pred'])\n",
    "\n",
    "        # Create heatmap\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=['No Disease', 'Disease'],\n",
    "                   yticklabels=['No Disease', 'Disease'],\n",
    "                   ax=axes[i])\n",
    "\n",
    "        axes[i].set_title(f'{metrics[\"model_name\"]}\\n'\n",
    "                         f'Accuracy: {metrics[\"accuracy\"]:.3f}')\n",
    "        axes[i].set_xlabel('Predicted')\n",
    "        axes[i].set_ylabel('Actual')\n",
    "\n",
    "        # Add percentage annotations\n",
    "        total = cm.sum()\n",
    "        for j in range(2):\n",
    "            for k in range(2):\n",
    "                percentage = cm[j, k] / total * 100\n",
    "                axes[i].text(k + 0.5, j + 0.7, f'({percentage:.1f}%)',\n",
    "                           ha='center', va='center', fontsize=10, color='red')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print confusion matrix interpretation\n",
    "    print(\"\\n=== CONFUSION MATRIX INTERPRETATION ===\")\n",
    "    for metrics in all_metrics:\n",
    "        cm = confusion_matrix(metrics['y_true'], metrics['y_pred'])\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "        print(f\"\\n{metrics['model_name']}:\")\n",
    "        print(f\"  True Negatives (Correctly predicted no disease): {tn}\")\n",
    "        print(f\"  False Positives (Incorrectly predicted disease): {fp}\")\n",
    "        print(f\"  False Negatives (Missed disease cases): {fn}\")\n",
    "        print(f\"  True Positives (Correctly predicted disease): {tp}\")\n",
    "        print(f\"  Sensitivity (Recall): {tp/(tp+fn):.3f}\")\n",
    "        print(f\"  Specificity: {tn/(tn+fp):.3f}\")\n",
    "\n",
    "plot_confusion_matrices(all_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a75202",
   "metadata": {},
   "source": [
    "# ROC Curve Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0276fac0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ROC Curve Analysis\n",
    "def plot_roc_curves(all_metrics):\n",
    "    \"\"\"\n",
    "    Plot ROC curves for all models\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    for metrics in all_metrics:\n",
    "        fpr, tpr, _ = roc_curve(metrics['y_true'], metrics['y_pred_proba'])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        plt.plot(fpr, tpr,\n",
    "                label=f\"{metrics['model_name']} (AUC = {roc_auc:.3f})\",\n",
    "                linewidth=2)\n",
    "\n",
    "    # Plot diagonal line\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "    plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "    plt.title('ROC Curves Comparison - Heart Disease Prediction')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    # Print AUC interpretation\n",
    "    print(\"\\n=== ROC-AUC INTERPRETATION ===\")\n",
    "    print(\"AUC (Area Under Curve) Interpretation:\")\n",
    "    print(\"- 1.0 = Perfect classifier\")\n",
    "    print(\"- 0.9-1.0 = Excellent\")\n",
    "    print(\"- 0.8-0.9 = Good\")\n",
    "    print(\"- 0.7-0.8 = Fair\")\n",
    "    print(\"- 0.6-0.7 = Poor\")\n",
    "    print(\"- 0.5 = Random guessing\")\n",
    "\n",
    "    for metrics in all_metrics:\n",
    "        fpr, tpr, _ = roc_curve(metrics['y_true'], metrics['y_pred_proba'])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        print(f\"\\n{metrics['model_name']}: AUC = {roc_auc:.3f}\")\n",
    "\n",
    "plot_roc_curves(all_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27fdd14",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Precision-Recall Curve Analysis\n",
    "def plot_precision_recall_curves(all_metrics):\n",
    "    \"\"\"\n",
    "    Plot Precision-Recall curves for all models\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    for metrics in all_metrics:\n",
    "        precision, recall, _ = precision_recall_curve(metrics['y_true'],\n",
    "                                                     metrics['y_pred_proba'])\n",
    "\n",
    "        # Calculate Average Precision\n",
    "        avg_precision = np.mean(precision)\n",
    "\n",
    "        plt.plot(recall, precision,\n",
    "                label=f\"{metrics['model_name']} (AP = {avg_precision:.3f})\",\n",
    "                linewidth=2)\n",
    "\n",
    "    # Add baseline (random classifier performance)\n",
    "    baseline = np.sum(metrics['y_true']) / len(metrics['y_true'])\n",
    "    plt.axhline(y=baseline, color='k', linestyle='--',\n",
    "               label=f'Baseline (Random): {baseline:.3f}')\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall (Sensitivity)')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curves - Heart Disease Prediction')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    # Print interpretation\n",
    "    print(\"\\n=== PRECISION-RECALL INTERPRETATION ===\")\n",
    "    print(\"For Heart Disease Prediction:\")\n",
    "    print(\"- High Precision = Few false alarms (healthy predicted as diseased)\")\n",
    "    print(\"- High Recall = Few missed cases (disease not detected)\")\n",
    "    print(\"- Average Precision = Summarizes the curve as a single number\")\n",
    "\n",
    "    # Calculate and display specific metrics\n",
    "    for metrics in all_metrics:\n",
    "        precision, recall, thresholds = precision_recall_curve(metrics['y_true'],\n",
    "                                                              metrics['y_pred_proba'])\n",
    "        avg_precision = np.mean(precision)\n",
    "        print(f\"\\n{metrics['model_name']}:\")\n",
    "        print(f\"  Average Precision: {avg_precision:.3f}\")\n",
    "        print(f\"  Precision: {metrics['precision']:.3f}\")\n",
    "        print(f\"  Recall: {metrics['recall']:.3f}\")\n",
    "\n",
    "plot_precision_recall_curves(all_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3253af4",
   "metadata": {},
   "source": [
    "# Advanced Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804d482c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Advanced Metrics Comparison\n",
    "def create_metrics_comparison_table(all_metrics):\n",
    "    \"\"\"\n",
    "    Create comprehensive metrics comparison table\n",
    "    \"\"\"\n",
    "    # Create detailed comparison dataframe\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Model': [m['model_name'] for m in all_metrics],\n",
    "        'Accuracy': [m['accuracy'] for m in all_metrics],\n",
    "        'Precision': [m['precision'] for m in all_metrics],\n",
    "        'Recall': [m['recall'] for m in all_metrics],\n",
    "        'F1-Score': [m['f1_score'] for m in all_metrics],\n",
    "        'MCC': [m['mcc'] for m in all_metrics],\n",
    "        'Kappa': [m['kappa'] for m in all_metrics]\n",
    "    })\n",
    "\n",
    "    # Add AUC scores\n",
    "    auc_scores = []\n",
    "    for metrics in all_metrics:\n",
    "        fpr, tpr, _ = roc_curve(metrics['y_true'], metrics['y_pred_proba'])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        auc_scores.append(roc_auc)\n",
    "\n",
    "    metrics_df['AUC'] = auc_scores\n",
    "\n",
    "    # Round to 3 decimal places\n",
    "    metrics_df = metrics_df.round(3)\n",
    "\n",
    "    print(\"=== COMPREHENSIVE METRICS COMPARISON ===\")\n",
    "    print(metrics_df.to_string(index=False))\n",
    "\n",
    "    # Highlight best values\n",
    "    print(\"\\n=== BEST PERFORMING MODELS ===\")\n",
    "    for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC', 'MCC']:\n",
    "        best_idx = metrics_df[metric].idxmax()\n",
    "        best_model = metrics_df.loc[best_idx, 'Model']\n",
    "        best_score = metrics_df.loc[best_idx, metric]\n",
    "        print(f\"{metric}: {best_model} ({best_score:.3f})\")\n",
    "\n",
    "    return metrics_df\n",
    "\n",
    "metrics_df = create_metrics_comparison_table(all_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f6063b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def interpret_medical_context(all_metrics):\n",
    "    \"\"\"\n",
    "    Interpret results in medical context\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MEDICAL CONTEXT INTERPRETATION\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for metrics in all_metrics:\n",
    "        cm = confusion_matrix(metrics['y_true'], metrics['y_pred'])\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "        print(f\"\\n{metrics['model_name']}:\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # Calculate medical metrics\n",
    "        sensitivity = tp / (tp + fn)  # True Positive Rate\n",
    "        specificity = tn / (tn + fp)  # True Negative Rate\n",
    "        ppv = tp / (tp + fp) if (tp + fp) > 0 else 0  # Positive Predictive Value\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # Negative Predictive Value\n",
    "\n",
    "        print(f\"Sensitivity (Recall): {sensitivity:.3f}\")\n",
    "        print(f\"  â†’ Ability to correctly identify patients WITH heart disease\")\n",
    "        print(f\"  â†’ {sensitivity*100:.1f}% of diseased patients were correctly identified\")\n",
    "\n",
    "        print(f\"\\nSpecificity: {specificity:.3f}\")\n",
    "        print(f\"  â†’ Ability to correctly identify patients WITHOUT heart disease\")\n",
    "        print(f\"  â†’ {specificity*100:.1f}% of healthy patients were correctly identified\")\n",
    "\n",
    "        print(f\"\\nPositive Predictive Value: {ppv:.3f}\")\n",
    "        print(f\"  â†’ When model predicts disease, it's correct {ppv*100:.1f}% of the time\")\n",
    "\n",
    "        print(f\"\\nNegative Predictive Value: {npv:.3f}\")\n",
    "        print(f\"  â†’ When model predicts no disease, it's correct {npv*100:.1f}% of the time\")\n",
    "\n",
    "        # Clinical implications\n",
    "        print(f\"\\nClinical Implications:\")\n",
    "        if sensitivity > 0.85:\n",
    "            print(\"   EXCELLENT: High sensitivity - few missed cases\")\n",
    "        elif sensitivity > 0.75:\n",
    "            print(\"   GOOD: Moderate sensitivity - some missed cases\")\n",
    "        else:\n",
    "            print(\"   POOR: Low sensitivity - many missed cases\")\n",
    "\n",
    "        if specificity > 0.85:\n",
    "            print(\"   EXCELLENT: High specificity - few false alarms\")\n",
    "        elif specificity > 0.75:\n",
    "            print(\"   GOOD: Moderate specificity - some false alarms\")\n",
    "        else:\n",
    "            print(\"   POOR: Low specificity - many false alarms\")\n",
    "\n",
    "interpret_medical_context(all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7c043b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Statistical Significance Testing\n",
    "from scipy import stats\n",
    "\n",
    "def perform_statistical_tests(all_metrics):\n",
    "    \"\"\"\n",
    "    Perform statistical significance tests\n",
    "    \"\"\"\n",
    "    print(\"\\n=== STATISTICAL SIGNIFICANCE TESTING ===\")\n",
    "\n",
    "    # McNemar's test for comparing two models\n",
    "    def mcnemar_test(y_true, model1_pred, model2_pred, model1_name, model2_name):\n",
    "        from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "        # Create contingency table\n",
    "        table = pd.crosstab(model1_pred == y_true, model2_pred == y_true)\n",
    "\n",
    "        if table.shape == (2, 2):\n",
    "            result = mcnemar(table, exact=True)\n",
    "            print(f\"\\nMcNemar's Test: {model1_name} vs {model2_name}\")\n",
    "            print(f\"  p-value: {result.pvalue:.4f}\")\n",
    "            if result.pvalue < 0.05:\n",
    "                print(\"  â†’ Statistically significant difference\")\n",
    "            else:\n",
    "                print(\"  â†’ No statistically significant difference\")\n",
    "\n",
    "    # Compare best model with others\n",
    "    best_metric = max(all_metrics, key=lambda x: x['f1_score'])\n",
    "    print(f\"\\nBest model: {best_metric['model_name']}\")\n",
    "\n",
    "    for metrics in all_metrics:\n",
    "        if metrics['model_name'] != best_metric['model_name']:\n",
    "            mcnemar_test(\n",
    "                metrics['y_true'],\n",
    "                best_metric['y_pred'],\n",
    "                metrics['y_pred'],\n",
    "                best_metric['model_name'],\n",
    "                metrics['model_name']\n",
    "            )\n",
    "\n",
    "perform_statistical_tests(all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2b101c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Error Analysis\n",
    "def detailed_error_analysis(best_metrics):\n",
    "    \"\"\"\n",
    "    Analyze prediction errors in detail\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== DETAILED ERROR ANALYSIS FOR {best_metrics['model_name']} ===\")\n",
    "\n",
    "    # Find misclassified cases\n",
    "    misclassified = X_test.copy()\n",
    "    misclassified['actual'] = best_metrics['y_true']\n",
    "    misclassified['predicted'] = best_metrics['y_pred']\n",
    "    misclassified['correct'] = best_metrics['y_true'] == best_metrics['y_pred']\n",
    "    misclassified = misclassified[~misclassified['correct']]\n",
    "\n",
    "    print(f\"Total misclassified cases: {len(misclassified)}\")\n",
    "\n",
    "    # Analyze false positives vs false negatives\n",
    "    false_positives = misclassified[misclassified['actual'] == 0]\n",
    "    false_negatives = misclassified[misclassified['actual'] == 1]\n",
    "\n",
    "    print(f\"False Positives (healthy predicted as diseased): {len(false_positives)}\")\n",
    "    print(f\"False Negatives (diseased predicted as healthy): {len(false_negatives)}\")\n",
    "\n",
    "    # Analyze characteristics of misclassified cases\n",
    "    if len(false_positives) > 0:\n",
    "        print(\"\\nFalse Positive Characteristics:\")\n",
    "        print(\"Average age:\", false_positives['age'].mean())\n",
    "        print(\"Average cholesterol:\", false_positives['chol'].mean())\n",
    "        print(\"Average resting BP:\", false_positives['trestbps'].mean())\n",
    "\n",
    "    if len(false_negatives) > 0:\n",
    "        print(\"\\nFalse Negative Characteristics:\")\n",
    "        print(\"Average age:\", false_negatives['age'].mean())\n",
    "        print(\"Average cholesterol:\", false_negatives['chol'].mean())\n",
    "        print(\"Average resting BP:\", false_negatives['trestbps'].mean())\n",
    "\n",
    "    # Create error analysis visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # Age distribution of errors\n",
    "    axes[0, 0].hist(false_positives['age'], alpha=0.7, label='False Positives', bins=10)\n",
    "    axes[0, 0].hist(false_negatives['age'], alpha=0.7, label='False Negatives', bins=10)\n",
    "    axes[0, 0].set_xlabel('Age')\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "    axes[0, 0].set_title('Age Distribution of Misclassified Cases')\n",
    "    axes[0, 0].legend()\n",
    "\n",
    "    # Cholesterol distribution of errors\n",
    "    axes[0, 1].hist(false_positives['chol'], alpha=0.7, label='False Positives', bins=10)\n",
    "    axes[0, 1].hist(false_negatives['chol'], alpha=0.7, label='False Negatives', bins=10)\n",
    "    axes[0, 1].set_xlabel('Cholesterol')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "    axes[0, 1].set_title('Cholesterol Distribution of Misclassified Cases')\n",
    "    axes[0, 1].legend()\n",
    "\n",
    "    # Error types pie chart\n",
    "    error_counts = [len(false_positives), len(false_negatives)]\n",
    "    error_labels = ['False Positives', 'False Negatives']\n",
    "    axes[1, 0].pie(error_counts, labels=error_labels, autopct='%1.1f%%',\n",
    "                   colors=['lightcoral', 'lightblue'])\n",
    "    axes[1, 0].set_title('Distribution of Error Types')\n",
    "\n",
    "    # Feature comparison between correct and incorrect predictions\n",
    "    correct_predictions = X_test[best_metrics['y_true'] == best_metrics['y_pred']]\n",
    "    incorrect_predictions = X_test[best_metrics['y_true'] != best_metrics['y_pred']]\n",
    "\n",
    "    feature_to_plot = 'age'  # You can change this to any feature\n",
    "    axes[1, 1].hist(correct_predictions[feature_to_plot], alpha=0.7,\n",
    "                    label='Correct Predictions', bins=15, density=True)\n",
    "    axes[1, 1].hist(incorrect_predictions[feature_to_plot], alpha=0.7,\n",
    "                    label='Incorrect Predictions', bins=15, density=True)\n",
    "    axes[1, 1].set_xlabel(feature_to_plot.title())\n",
    "    axes[1, 1].set_ylabel('Density')\n",
    "    axes[1, 1].set_title(f'{feature_to_plot.title()} Distribution: Correct vs Incorrect')\n",
    "    axes[1, 1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run error analysis for the best model\n",
    "best_metrics = max(all_metrics, key=lambda x: x['f1_score'])\n",
    "detailed_error_analysis(best_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5835f85",
   "metadata": {},
   "source": [
    "## creating final report and save in a report.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f086b1e3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_evaluation_report(all_metrics, model_name):\n",
    "    \"\"\"\n",
    "    Create a comprehensive evaluation report\n",
    "    \"\"\"\n",
    "    report = f\"\"\"\n",
    "    HEART DISEASE PREDICTION MODEL - EVALUATION REPORT\n",
    "    ===================================================\n",
    "    \n",
    "    Model: {model_name}\n",
    "    Evaluation Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}\n",
    "    \n",
    "    1. OVERALL PERFORMANCE\n",
    "    ----------------------\n",
    "    Accuracy: {max(all_metrics, key=lambda x: x['accuracy'])['accuracy']:.3f}\n",
    "    Precision: {max(all_metrics, key=lambda x: x['precision'])['precision']:.3f}\n",
    "    Recall: {max(all_metrics, key=lambda x: x['recall'])['recall']:.3f}\n",
    "    F1-Score: {max(all_metrics, key=lambda x: x['f1_score'])['f1_score']:.3f}\n",
    "    AUC-ROC: {max(all_metrics, key=lambda x: x.get('auc', 0)):.3f}\n",
    "    \n",
    "    2. CLINICAL PERFORMANCE\n",
    "    -----------------------\n",
    "    Sensitivity (Disease Detection Rate): {max(all_metrics, key=lambda x: x['recall'])['recall']:.3f}\n",
    "    Specificity (Healthy Identification Rate): {max(all_metrics, key=lambda x: x.get('specificity', 0)):.3f}\n",
    "    \n",
    "    3. MODEL COMPARISON\n",
    "    -------------------\n",
    "    Best Overall Model: {max(all_metrics, key=lambda x: x['f1_score'])['model_name']}\n",
    "    Most Accurate Model: {max(all_metrics, key=lambda x: x['accuracy'])['model_name']}\n",
    "    Most Sensitive Model: {max(all_metrics, key=lambda x: x['recall'])['model_name']}\n",
    "    \n",
    "    4. CLINICAL INTERPRETATION\n",
    "    --------------------------\n",
    "    - The model can correctly identify {max(all_metrics, key=lambda x: x['recall'])['recall']*100:.1f}% of patients with heart disease\n",
    "    - The model has a precision of {max(all_metrics, key=lambda x: x['precision'])['precision']*100:.1f}% when predicting heart disease\n",
    "    - Out of every 100 positive predictions, {max(all_metrics, key=lambda x: x['precision'])['precision']*100:.1f} are actually correct\n",
    "    \n",
    "    5. RECOMMENDATIONS\n",
    "    ------------------\n",
    "    - Model is {'suitable' if max(all_metrics, key=lambda x: x['f1_score'])['f1_score'] > 0.8 else 'needs improvement'} for clinical deployment\n",
    "    - Consider {'further tuning' if max(all_metrics, key=lambda x: x['f1_score'])['f1_score'] < 0.85 else 'validation on external dataset'}\n",
    "    \"\"\"\n",
    "    \n",
    "    print(report)\n",
    "    \n",
    "    # Save report to file\n",
    "    with open('model_evaluation_report.txt', 'w') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    print(\"\\n   Report saved as 'model_evaluation_report.txt'\")\n",
    "\n",
    "# Create final report\n",
    "best_model_name = max(all_metrics, key=lambda x: x['f1_score'])['model_name']\n",
    "create_evaluation_report(all_metrics, best_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507e9e81",
   "metadata": {},
   "source": [
    "# Common evaluation issues and solutions\n",
    "## issue 1: \"My model shows perfect performance\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83b10ee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check for data leakage\n",
    "print(\"Check if you're leaking target information:\")\n",
    "print(\"X_train columns:\", X_train.columns)\n",
    "print(\"Make sure target variable is not in features\")\n",
    "\n",
    "# Check train-test split\n",
    "print(\"Training accuracy:\", model.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", model.score(X_test, y_test))\n",
    "# If training >> test, you might be overfitting"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
